# -*- coding: utf-8 -*-
"""NearestNeighbors_TAsession_tutorcopy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yZ9k-s3inMR0JV2iXm7yGtrV4zeeMk4E

# Forest cover classification via K-Nearest Neighbors

In this project, you have to predict the class of forest cover (the predominant kind of tree cover) from strictly cartographic and environment variables.

The actual forest cover type for a given observation (30 x 30 meter cell) was determined from US Forest Service (USFS) Region 2 Resource Information System (RIS) data. Independent variables were derived from data originally obtained from US Geological Survey (USGS) and USFS data. Data is in raw form (not scaled) and contains categorical data for qualitative independent variables (wilderness areas and soil types).

You have further details on the data at *covertype.info* file and at https://archive.ics.uci.edu/ml/datasets/Covertype

Be aware that the final dataset has been slighly modified from the original source data.

You can follow these **steps**:
1. Explore and understand the *MultiClass_Train.csv* dataset. 
2. Create *dummy variables* for relevant categorical features.
3. Reformat the Class_type variable into a binary one, being class #7 the target variable versus the others.
4. Build a KNN model for predicting class #7 and test it on the same input data. Tune KNN hyperparameters via *cross-validation*. 
5. Try to improve your model using methods to tackle class imbalance.
"""

# if running on colab

from google.colab import drive
drive.mount('/content/drive')

ls

ls

# !pip install imblearn
# !pip install delayed

# imports

import pandas as pd
from sklearn import preprocessing
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors 
from sklearn.model_selection import train_test_split
from utils.helper_functions import *
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix
from sklearn.linear_model import LogisticRegression
from imblearn.over_sampling import RandomOverSampler

# paths
data_path = './Data/'

"""### Step 1: Read and explore data"""

# Suggestions:

# Read the documentation about the data
# Look at the column names and at the first rows of the data
# Check summary statistics of continuous variables
# Check categories of categorical variables
# Look at the balance of the categories of the dependent variable

df = pd.read_csv(data_path + 'MultiClass_Train.csv')

df.shape

df.head()

df_cols = df.columns
df.columns

# <table style="width:100%">
#     <tr><th><img src = "./images_2/variable_description.png" ></th>
# </tr></table>

"""![](https://drive.google.com/uc?export=view&id=1j_o0cxhzIGn3peUyt41tQylL_Di7iR1F)"""

cont_feat = ['Elevation', 'Aspect', 'Slope', 'Horiz_dist_hydro',
       'Vertical_dist_hydro', 'Horiz_dist_roadways', 'Hillshade_9am',
       'Hillshade_Noon', 'Hillshade_3pm', 'Horiz_dist_firepoints']

cat_feat = ['Wilderness_Area', 'Soil_Type']

df[cont_feat].describe()

for c in cat_feat + ['Cover_Type']:
    print(f'{c}:')
    print(sorted(df[c].unique()))

df[['Cover_Type']].value_counts()

"""### Step 2: Create dummies for relevant features """

df = pd.get_dummies(df, columns=cat_feat, drop_first=False)

df.columns

"""### Step 3: Reformat the data to have an binary class target (class #7 is the target to predict)"""

df['is_cover_7'] = (df['Cover_Type']==7).astype(int)

df[['Cover_Type', 'is_cover_7']].iloc[1000:1020]

df.columns

df = df.drop(['Cover_Type'], axis=1).copy()

"""### Step 4: Build a binary model to predict class #7

#### a): Prepare the data
"""

# Suggestions:

# Train-test split features and dependent variable
# Scale continuous variables

X, y = df.drop(['is_cover_7'], axis=1), df['is_cover_7']

y.value_counts()

# train-test split
X_train, X_test, y_train, y_test = train_test_split(X.copy(), y.copy(), random_state=3, test_size=0.25)

y_train.value_counts(),y_test.value_counts()

# scale continuous variables
scaler = preprocessing.StandardScaler().fit(X_train[cont_feat].copy())

X_train[cont_feat] = scaler.transform(X_train[cont_feat].copy())
X_test[cont_feat] = scaler.transform(X_test[cont_feat].copy())

pd.concat([X_train[cont_feat].mean(), X_train[cont_feat].std(),
          X_test[cont_feat].mean(), X_test[cont_feat].std(),], axis=1, keys=['train_mean', 'train_std',
                                                                            'test_mean', 'test_std'])

"""#### b): Grid-search KNN parameters"""

# Suggestions:

# Choose a manageable but meaningful set of hyperparameters to search on

"""N.B. Use a scoring metric that makes sense in a context of class imbalance. For a list of scoring rules, check 
https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter. Some scoring rules that you might want to try: balanced_accuracy, f1, roc_auc.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# MyKNN = KNeighborsClassifier(algorithm='brute') #, metric=lambda X, Y: mydist(X, Y))
# 
# k_vals = [1, 2, 5, 10, 20] 
# weights = ['uniform', 'distance']
# 
# grid_values = {'n_neighbors':k_vals, 'weights':weights}
# grid_knn = GridSearchCV(MyKNN, param_grid=grid_values, scoring='f1', cv=5) 
# 
# grid_knn.fit(X_train, y_train)

print('best parameters:', grid_knn.best_params_)
print('best score:', grid_knn.best_score_)

# GridSearch_table_plot(grid_knn, "n_neighbors", negative=False, display_all_params=False)

"""#### c): Test the model on the test split"""

# Suggestions:

# Check the Confusion Matrix
# Check various performance metrics and try to make sense of them

# predict y based on best parameters
y_pred = grid_knn.predict(X_test)

"""**Some scoring metrics:**

+ **Accuracy score**: number of correctly classified observations over the total number of observations, $\frac{TP+TN}{\textit{Total obs.}}$.

+ **Precision score**: ratio of correctly predicted positive observations to the total predicted positive observations, $\frac{TP}{\text{TP + FP}}$. To improve precision, we want to lower the number of observations we predicted to be positive when they were not (False Positive). 

+ **Recall score**: ratio of correctly predicted positive observations to the total true positive observations, $\frac{TP}{\text{TP + FN}}$. To improve recall, we want to lower the number of observations we predicted to be negative when they were not (False Negative). It is also called *sensitivity* or True Positive Rate (TPR).

+ **F1 score**:  harmonic mean of the precision and recall, $\frac{2*Precision*Recall}{\textit{Precision + Recall}}$. An F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal.
"""

# <table style="width:30%">
#     <tr><th><img src = "./images_2/cm_table.png" ></th>
# </tr></table>

"""![](https://drive.google.com/uc?export=view&id=1hWfXYDon9crdov2FmTMJw44cQE8-UCGY)

+ **Roc-auc score**: The Receiver Operator Characteristic (ROC) curve is a probability curve that plots the Recall (or TPR) against the False Positive Rate (FPR )** at various threshold values. The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve. The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.

    \** FPR = 1 - False Negative Rate (FNR or Specificity). The FNR tells us what proportion of the positive class got incorrectly classified by the classifier, $\frac{FN}{\text{TP + FN}}$
"""

#    <table style="width:30%">
#         <tr><th><img src = "./images_2/ROC_AUC.png" ></th>
#     </tr></table>

"""![](https://drive.google.com/uc?export=view&id=1gsKGJobPfLTxCm2f4fAulw_LkmwKU2Fh)"""

#Confusion matrix
print("Confusion matrix")
cm = confusion_matrix(y_test, y_pred)
plot_confusion_matrix(cm, ['Other','Target'])

print('Accuracy Score : ' + str(accuracy_score(y_test, y_pred))) 
print('Precision Score : ' + str(precision_score(y_test, y_pred)))
print('Recall Score : ' + str(recall_score(y_test, y_pred)))
print('F1 Score : ' + str(f1_score(y_test, y_pred)))
print('ROC AUC Score : ' + str(roc_auc_score(y_test, y_pred)))

#Distribution of y test
print('y actual : \n' +  str(y_test.value_counts()))
#Distribution of y predicted
print('y predicted : \n' + str(pd.Series(y_pred).value_counts()))

"""### Step 5: Try to improve your model using methods to tackle class imbalance."""

# Suggestions: 
# Re-do steps 1) to 3) with over-sampling and see whether you obtain an improvement

"""N.B. For tools to implement different over-sampling techniques check: https://imbalanced-learn.org/stable/over_sampling.html. Techniques you might want to check: random over-sampling, SMOTE, ADASYN. """

# random over-sampling: generate new samples by randomly sampling with replacement the underrepresented class
sampling_strategy = 0.5
rus = RandomOverSampler(sampling_strategy=sampling_strategy)
X_res, y_res = rus.fit_resample(X, y)

y_res.value_counts()

# train-test split
X_train_res, X_test_res, y_train_res, y_test_res = train_test_split(X_res.copy(), y_res.copy(), 
                                                                    random_state=3, test_size=0.25)

# scale continuous variables
scaler = preprocessing.StandardScaler().fit(X_train_res[cont_feat].copy())

X_train_res[cont_feat] = scaler.transform(X_train_res[cont_feat].copy())
X_test_res[cont_feat] = scaler.transform(X_test_res[cont_feat].copy())

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # grid-search
# MyKNN_res = KNeighborsClassifier(algorithm='brute') #, metric=lambda X, Y: mydist(X, Y))
# 
# k_vals = [1, 2, 5, 10, 20] 
# weights = ['uniform', 'distance']
# 
# grid_values = {'n_neighbors':k_vals, 'weights':weights}
# grid_knn_res = GridSearchCV(MyKNN_res, param_grid=grid_values, scoring='f1', cv=5) 
# 
# grid_knn_res.fit(X_train_res, y_train_res)

print('best parameters:', grid_knn_res.best_params_)
print('best score:', grid_knn_res.best_score_)

# GridSearch_table_plot(grid_knn_res, "n_neighbors", negative=False, display_all_params=False)

# predict y based on best parameters
y_pred_res = grid_knn_res.predict(X_test)

#Confusion matrix
print("Confusion matrix")
cm = confusion_matrix(y_test, y_pred_res)
plot_confusion_matrix(cm, ['Other','Target'])

print('Accuracy Score : ' + str(accuracy_score(y_test, y_pred_res))) 
print('Precision Score : ' + str(precision_score(y_test, y_pred_res)))
print('Recall Score : ' + str(recall_score(y_test, y_pred_res)))
print('F1 Score : ' + str(f1_score(y_test, y_pred_res)))
print('ROC AUC Score : ' + str(roc_auc_score(y_test, y_pred_res)))

#Distribution of y test
print('y actual : \n' +  str(y_test.value_counts()))
#Distribution of y predicted
print('y predicted : \n' + str(pd.Series(y_pred_res).value_counts()))