{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLdXjgq3dNU8"
   },
   "source": [
    "<img src = \"../../Data/bgsedsc_0.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNDWhzFAdNVA"
   },
   "source": [
    "# Project: Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8jKHCjUdNVA"
   },
   "source": [
    "## Programming project: probability of death\n",
    "\n",
    "In this project, you have to predict the probability of death of a patient that is entering an ICU (Intensive Care Unit).\n",
    "\n",
    "The dataset comes from MIMIC project (https://mimic.physionet.org/). MIMIC-III (Medical Information Mart for Intensive Care III) is a large, freely-available database comprising deidentified health-related data associated with over forty thousand patients who stayed in critical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012.\n",
    "\n",
    "Each row of *mimic_train.csv* correponds to one ICU stay (*hadm_id*+*icustay_id*) of one patient (*subject_id*). Column HOSPITAL_EXPIRE_FLAG is the indicator of death (=1) as a result of the current hospital stay; this is the outcome to predict in our modelling exercise.\n",
    "The remaining columns correspond to vitals of each patient (when entering the ICU), plus some general characteristics (age, gender, etc.), and their explanation can be found at *mimic_patient_metadata.csv*. \n",
    "\n",
    "Note that the main cause/disease of patient contidition is embedded as a code at *ICD9_diagnosis* column. The meaning of this code can be found at *MIMIC_metadata_diagnose.csv*. **But** this is only the main one; a patient can have co-occurrent diseases (comorbidities). These secondary codes can be found at *extra_data/MIMIC_diagnoses.csv*.\n",
    "\n",
    "Don't use features that you don't know the first day a patient enters the ICU, such as LOS.\n",
    "\n",
    "As performance metric, you can use *AUC* for the binary classification case, but feel free to report as well any other metric if you can justify that is particularly suitable for this case.\n",
    "\n",
    "Main tasks are:\n",
    "+ Using *mimic_train.csv* file build a predictive model for *HOSPITAL_EXPIRE_FLAG* .\n",
    "+ For this analysis there is an extra test dataset, *mimic_test.csv*. Apply your final model to this extra dataset and submit to Kaggle competition to obtain accuracy of prediction (follow the requested format).\n",
    "\n",
    "Try to optimize hyperparameters of your SVM model.\n",
    "\n",
    "You can follow those **steps** in your first implementation:\n",
    "1. *Explore* and understand the dataset. \n",
    "2. Manage missing data.\n",
    "2. Manage categorial features. E.g. create *dummy variables* for relevant categorical features, or build an ad hoc distance function.\n",
    "3. Build a prediction model. Try to improve it using methods to tackle class imbalance.\n",
    "5. Assess expected accuracy  of previous models using *cross-validation*. \n",
    "6. Test the performance on the test file by submitting to Kaggle, following same preparation steps (missing data, dummies, etc). Remember that you should be able to yield a prediction for all the rows of the test dataset.\n",
    "\n",
    "For the in-class version, feel free to reduce the training dataset if you experience computational constraints.\n",
    "\n",
    "## Main criteria for IN_CLASS grading\n",
    "The weighting of these components will vary between the in-class and extended projects:\n",
    "+ Code runs - 15%\n",
    "+ Data preparation - 20%\n",
    "+ SVMs method(s) have been used - 20%\n",
    "+ Probability of death for each test patient is computed - 15%\n",
    "+ Accuracy itself - 15%\n",
    "+ Hyperparameter optimization - 10%\n",
    "+ Class imbalance management - 5%\n",
    "+ Neat and understandable code, with some titles and comments - 0%\n",
    "+ Improved methods from what we discussed in class (properly explained/justified) - 0%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hjlNkbSSdNVD",
    "outputId": "a865e228-c4ee-49ed-e450-65b8c2b70d24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "os.chdir('/content/drive/MyDrive/CML_2_Projects/Project 2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "Kz1PvNIgZx_O"
   },
   "outputs": [],
   "source": [
    "from utils import helper_functions\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_predict as cvp\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "g2veHl8zjmQy"
   },
   "outputs": [],
   "source": [
    "def reweight_binary(pi,q1=0.5,r1=0.5):\n",
    "    r0 = 1-r1\n",
    "    q0 = 1-q1\n",
    "    tot = pi*(q1/r1)+(1-pi)*(q0/r0)\n",
    "    w = pi*(q1/r1)\n",
    "    w /= tot\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "cOydmY0xaBGb"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('mimic_train.csv')\n",
    "test_data = pd.read_csv('mimic_test_death.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "V81H-QGuaDIH"
   },
   "outputs": [],
   "source": [
    "#assign features\n",
    "features_to_drop = ['LOS', 'Diff']\n",
    "identifiers = ['subject_id', 'hadm_id', 'icustay_id']\n",
    "numerical_features = ['HeartRate_Min', 'HeartRate_Max', 'HeartRate_Mean', 'SysBP_Min',\n",
    "       'SysBP_Max', 'SysBP_Mean', 'DiasBP_Min', 'DiasBP_Max', 'DiasBP_Mean',\n",
    "       'MeanBP_Min', 'MeanBP_Max', 'MeanBP_Mean', 'RespRate_Min',\n",
    "       'RespRate_Max', 'RespRate_Mean', 'TempC_Min', 'TempC_Max', 'TempC_Mean',\n",
    "       'SpO2_Min', 'SpO2_Max', 'SpO2_Mean', 'Glucose_Min', 'Glucose_Max',\n",
    "       'Glucose_Mean']\n",
    "categorical_features = ['GENDER', 'DOB', 'ADMITTIME', 'ADMISSION_TYPE', 'INSURANCE', 'RELIGION',\n",
    "       'MARITAL_STATUS', 'ETHNICITY', 'DIAGNOSIS', 'ICD9_diagnosis',\n",
    "       'FIRST_CAREUNIT']\n",
    "target = ['HOSPITAL_EXPIRE_FLAG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "YoziQ32KaIrD"
   },
   "outputs": [],
   "source": [
    "#drop irrelevant columns\n",
    "train_data = train_data.drop(features_to_drop, axis=1)\n",
    "test_data = test_data.drop('Diff', axis=1)\n",
    "#note that offending columns are not in test set so no need to drop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EJBvLf03bE7B",
    "outputId": "b8f23557-e7ae-471b-e38c-d03821a13261"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HOSPITAL_EXPIRE_FLAG       0\n",
       "subject_id                 0\n",
       "hadm_id                    0\n",
       "icustay_id                 0\n",
       "HeartRate_Min           2187\n",
       "HeartRate_Max           2187\n",
       "HeartRate_Mean          2187\n",
       "SysBP_Min               2208\n",
       "SysBP_Max               2208\n",
       "SysBP_Mean              2208\n",
       "DiasBP_Min              2209\n",
       "DiasBP_Max              2209\n",
       "DiasBP_Mean             2209\n",
       "MeanBP_Min              2186\n",
       "MeanBP_Max              2186\n",
       "MeanBP_Mean             2186\n",
       "RespRate_Min            2189\n",
       "RespRate_Max            2189\n",
       "RespRate_Mean           2189\n",
       "TempC_Min               2497\n",
       "TempC_Max               2497\n",
       "TempC_Mean              2497\n",
       "SpO2_Min                2203\n",
       "SpO2_Max                2203\n",
       "SpO2_Mean               2203\n",
       "Glucose_Min              253\n",
       "Glucose_Max              253\n",
       "Glucose_Mean             253\n",
       "GENDER                     0\n",
       "DOB                        0\n",
       "ADMITTIME                  0\n",
       "ADMISSION_TYPE             0\n",
       "INSURANCE                  0\n",
       "RELIGION                   0\n",
       "MARITAL_STATUS           722\n",
       "ETHNICITY                  0\n",
       "DIAGNOSIS                  0\n",
       "ICD9_diagnosis             0\n",
       "FIRST_CAREUNIT             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77y7Ue-BbFk6",
    "outputId": "8e45935a-ef5c-40ec-82c5-0ac77535ea34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject_id          0\n",
       "hadm_id             0\n",
       "icustay_id          0\n",
       "HeartRate_Min     545\n",
       "HeartRate_Max     545\n",
       "HeartRate_Mean    545\n",
       "SysBP_Min         551\n",
       "SysBP_Max         551\n",
       "SysBP_Mean        551\n",
       "DiasBP_Min        552\n",
       "DiasBP_Max        552\n",
       "DiasBP_Mean       552\n",
       "MeanBP_Min        547\n",
       "MeanBP_Max        547\n",
       "MeanBP_Mean       547\n",
       "RespRate_Min      546\n",
       "RespRate_Max      546\n",
       "RespRate_Mean     546\n",
       "TempC_Min         638\n",
       "TempC_Max         638\n",
       "TempC_Mean        638\n",
       "SpO2_Min          551\n",
       "SpO2_Max          551\n",
       "SpO2_Mean         551\n",
       "Glucose_Min        58\n",
       "Glucose_Max        58\n",
       "Glucose_Mean       58\n",
       "GENDER              0\n",
       "DOB                 0\n",
       "ADMITTIME           0\n",
       "ADMISSION_TYPE      0\n",
       "INSURANCE           0\n",
       "RELIGION            0\n",
       "MARITAL_STATUS    180\n",
       "ETHNICITY           0\n",
       "DIAGNOSIS           0\n",
       "ICD9_diagnosis      0\n",
       "FIRST_CAREUNIT      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "baiflHlgaw7m"
   },
   "outputs": [],
   "source": [
    "#basic imputation for numerical features\n",
    "imp_num = SimpleImputer(strategy=\"mean\")\n",
    "imp_num.fit(train_data[numerical_features])\n",
    "train_data[numerical_features] = imp_num.transform(train_data[numerical_features])\n",
    "test_data[numerical_features] = imp_num.transform(test_data[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "bb8fdorcbNMt"
   },
   "outputs": [],
   "source": [
    "#impute for categorical features (note it's only marital status)\n",
    "categorical_features\n",
    "imp_cat = SimpleImputer(strategy=\"most_frequent\")\n",
    "imp_cat.fit(train_data[categorical_features])\n",
    "train_data[categorical_features] = imp_cat.transform(train_data[categorical_features])\n",
    "test_data[categorical_features] = imp_cat.transform(test_data[categorical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pMMUrvCra4af",
    "outputId": "bc6d700a-60e0-4b0a-88b5-54a5f0a69aaf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GENDER                2\n",
       "DOB               14007\n",
       "ADMITTIME         19714\n",
       "ADMISSION_TYPE        3\n",
       "INSURANCE             5\n",
       "RELIGION             17\n",
       "MARITAL_STATUS        7\n",
       "ETHNICITY            41\n",
       "DIAGNOSIS          6193\n",
       "ICD9_diagnosis     1853\n",
       "FIRST_CAREUNIT        5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[categorical_features].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7iQ6jTLbne-"
   },
   "outputs": [],
   "source": [
    "#short on time to make an age variable so drop these\n",
    "categorical_features = categorical_features.drop('DOB', 'ADMITTIME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "VqC8RXnqbzD0"
   },
   "outputs": [],
   "source": [
    "#scale features (only numerical features)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data[numerical_features])\n",
    "train_data[numerical_features] = scaler.transform(train_data[numerical_features])\n",
    "test_data[numerical_features] = scaler.transform(test_data[numerical_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HeFBhPBb3Lq"
   },
   "source": [
    "First implementation using linear SVC and only numerical features\n",
    "\n",
    "Note that below I comment out a reasonable grid search due to time constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mq1DNTnHb2Q0",
    "outputId": "cd9aee4a-e05e-427e-91fd-c176e3c95997"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "linear_svc = SVC(kernel='linear', probability = True, C = 1) #class_weight imbalance not addressed \n",
    "#grid_values = {'C':[0.1, 1, 10, 100]}  \n",
    "#grid_linear_svc = GridSearchCV(linear_svc, param_grid = grid_values,scoring = 'roc_auc', cv=5)\n",
    "linear_svc.fit(train_data[numerical_features], train_data[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "QspZ8JmMdMYv"
   },
   "outputs": [],
   "source": [
    "y_hat_prob = linear_svc.predict_proba(train_data[numerical_features]) \n",
    "#note that I am aware that sklearn documentation warns that probabilites and point predictions\n",
    "#may not completely align, but for the purposes of the in-class I continue with this approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "NwFr4FX4h6qf"
   },
   "outputs": [],
   "source": [
    "#get predictions for kaggle\n",
    "y_hat_test = linear_svc.predict_proba(test_data[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j3Gg7nxjcJvI",
    "outputId": "10cca45d-22c6-4d1b-c7d1-db4451d99977"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6863399599321929"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(train_data[target], y_hat_prob[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmD9jgJggY6k"
   },
   "outputs": [],
   "source": [
    "#out of sample prediction \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "y_hat_cv = cross_val_predict(linear_svc, train_data[numerical_features], train_data[target], cv = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Azq5vOUglYf"
   },
   "outputs": [],
   "source": [
    "roc_auc_score(train_data[target], y_hat_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "H1bimOh_joEs",
    "outputId": "4edd83f3-daf4-4726-f755-e850947b9ae6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-56677d1a6611>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mq1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mr1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0my_hat_test_balanced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat_test_balanced\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreweight_binary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'reweight_binary' is not defined"
     ]
    }
   ],
   "source": [
    "#try for balanced class_weight\n",
    "linear_svc_balanced = SVC(kernel='linear', probability = True, C = 1, class_weight = 'balanced') #would usually grid search for C\n",
    "linear_svc_balanced.fit(train_data[numerical_features], train_data[target])\n",
    "y_hat_prob_balanced = linear_svc_balanced.predict_proba(train_data[numerical_features]) \n",
    "y_hat_test_balanced = linear_svc_balanced.predict_proba(test_data[numerical_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "4yAOrI1Km4E1"
   },
   "outputs": [],
   "source": [
    "#reweight probabilities \n",
    "\n",
    "q1 = train_data[target].sum()/len(train_data[target])\n",
    "r1 = 0.5\n",
    "y_hat_test_balanced = pd.DataFrame(y_hat_test_balanced[:, 1]).apply(reweight_binary,args=(q1,r1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "-mjev1CUkrpu",
    "outputId": "36e003f0-2790-4316-e087-a0458b56c3f2"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-758f87cb855f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat_test_balanced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     if y_type == \"multiclass\" or (\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"allow-nan\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    114\u001b[0m             raise ValueError(\n\u001b[1;32m    115\u001b[0m                 msg_err.format(\n\u001b[0;32m--> 116\u001b[0;31m                     \u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                 )\n\u001b[1;32m    118\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "roc_auc_score(train_data[target], y_hat_test_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGT7l0WbcjTn"
   },
   "source": [
    "Second simple implementation using non-linear SVC and only numerical features\n",
    "\n",
    "Note that below I comment out a reasonable grid search due to time constraints. \n",
    "\n",
    "Fit didn't run in time, but code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pVv9LXtZce3P",
    "outputId": "1eb4920c-9cbf-4ec9-b48d-17fbc4a68a3e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "rbf_svc = SVC(kernel='rbf', probability = True, C=1, gamma=0.5) #on reflection should have probably left as the default gamma \n",
    "#grid_values = {'C':[0.1, 1, 10], 'gamma':[0.01,0.1,0.2, 0.5, 0.8]} #will try more values for gamma in extended project\n",
    "#grid_rbf_svc = GridSearchCV(rbf_svc, param_grid=grid_values, scoring = 'roc_auc', cv = 5)\n",
    "rbf_svc.fit(train_data[numerical_features], train_data[target])\n",
    "y_hat_prob = rbf_svc.predict_proba(train_data[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yxs_zovogCWD",
    "outputId": "0347264d-f943-45d5-c895-b65a4766bbdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9525999797590872"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(train_data[target], y_hat_prob[:, 1])\n",
    "#bit of a crazy roc_auc_score. I assume some overfitting is going on (would check with cross-val), but purpose was to demonstrate I understood\n",
    "#how to implement the non-linear SVC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "MTDBSrJ8iFa3"
   },
   "outputs": [],
   "source": [
    "#get predictions for kaggle\n",
    "y_hat_test = rbf_svc.predict_proba(test_data[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kr7eQui5gt4y"
   },
   "outputs": [],
   "source": [
    "#out of sample prediction\n",
    "y_hat_cv = cross_val_predict(rbf_svc, train_data[numerical_features], train_data[target], cv = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHpBwWFsgw7z"
   },
   "outputs": [],
   "source": [
    "roc_auc_score(train_data[target], y_hat_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ar_k9Z0NdNVK"
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gzqPU6AfDje"
   },
   "source": [
    "### Kaggle Predictions Submissions\n",
    "\n",
    "Once you have produced testset predictions you can submit these to <i> kaggle </i> in order to see how your model performs. \n",
    "\n",
    "The following code provides an example of generating a <i> .csv </i> file to submit to kaggle\n",
    "1) create a pandas dataframe with two columns, one with the test set \"icustay_id\"'s and the other with your predicted \"HOSPITAL_EXPIRE_FLAG\" for that observation\n",
    "\n",
    "2) use the <i> .to_csv </i> pandas method to create a csv file. The <i> index = False </i> is important to ensure the <i> .csv </i> is in the format kaggle expects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "k25Iy1t_fEet"
   },
   "outputs": [],
   "source": [
    "# Produce .csv for kaggle testing \n",
    "test_predictions_submit = pd.DataFrame({\"icustay_id\": test_data[\"icustay_id\"], \"HOSPITAL_EXPIRE_FLAG\": y_hat_test_balanced[:, 1]})\n",
    "test_predictions_submit.to_csv(\"test_predictions_submit.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Benjamin Seimon - SVM_project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
