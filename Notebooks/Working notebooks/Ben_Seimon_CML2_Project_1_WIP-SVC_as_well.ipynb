{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "807002bd",
   "metadata": {},
   "source": [
    "# Project: (K-) Nearest Neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90673830",
   "metadata": {},
   "source": [
    "## Programming project: probability of death\n",
    "\n",
    "In this project, you have to predict the probability of death of a patient that is entering an ICU (Intensive Care Unit).\n",
    "\n",
    "The dataset comes from MIMIC project (https://mimic.physionet.org/). MIMIC-III (Medical Information Mart for Intensive Care III) is a large, freely-available database comprising deidentified health-related data associated with over forty thousand patients who stayed in critical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012.\n",
    "\n",
    "Each row of *mimic_train.csv* correponds to one ICU stay (*hadm_id*+*icustay_id*) of one patient (*subject_id*). Column HOSPITAL_EXPIRE_FLAG is the indicator of death (=1) as a result of the current hospital stay; this is the outcome to predict in our modelling exercise.\n",
    "The remaining columns correspond to vitals of each patient (when entering the ICU), plus some general characteristics (age, gender, etc.), and their explanation can be found at *mimic_patient_metadata.csv*. \n",
    "\n",
    "Please don't use any feature that you infer you don't know the first day of a patient in an ICU.\n",
    "\n",
    "Note that the main cause/disease of patient condition is embedded as a code at *ICD9_diagnosis* column. The meaning of this code can be found at *MIMIC_metadata_diagnose.csv*. **But** this is only the main one; a patient can have co-occurrent diseases (comorbidities). These secondary codes can be found at *extra_data/MIMIC_diagnoses.csv*.\n",
    "\n",
    "As performance metric, you can use *AUC* for the binary classification case, but feel free to report as well any other metric if you can justify that is particularly suitable for this case.\n",
    "\n",
    "Main tasks are:\n",
    "+ Using *mimic_train.csv* file build a predictive model for *HOSPITAL_EXPIRE_FLAG* .\n",
    "+ For this analysis there is an extra test dataset, *mimic_test_death.csv*. Apply your final model to this extra dataset and generate predictions following the same format as *mimic_kaggle_death_sample_submission.csv*. Once ready, you can submit to our Kaggle competition and iterate to improve the accuracy.\n",
    "\n",
    "As a *bonus*, try different algorithms for neighbor search and for distance, and justify final selection. Try also different weights to cope with class imbalance and also to balance neighbor proximity. Try to assess somehow confidence interval of predictions.\n",
    "\n",
    "You can follow those **steps** in your first implementation:\n",
    "1. *Explore* and understand the dataset. \n",
    "2. Manage missing data.\n",
    "2. Manage categorial features. E.g. create *dummy variables* for relevant categorical features, or build an ad hoc distance function.\n",
    "3. Build a prediction model. Try to improve it using methods to tackle class imbalance.\n",
    "5. Assess expected accuracy  of previous models using *cross-validation*. \n",
    "6. Test the performance on the test file and report accuracy, following same preparation steps (missing data, dummies, etc). Remember that you should be able to yield a prediction for all the rows of the test dataset.\n",
    "\n",
    "Feel free to reduce the training dataset if you experience computational constraints.\n",
    "\n",
    "## Main criteria for IN_CLASS grading\n",
    "The weighting of these components will vary between the in-class and extended projects:\n",
    "+ Code runs - 20%\n",
    "+ Data preparation - 35%\n",
    "+ Nearest neighbor method(s) have been used - 15%\n",
    "+ Probability of death for each test patient is computed - 10%\n",
    "+ Accuracy of predictions for test patients is calculated (kaggle) - 10%\n",
    "+ Hyperparameter optimization - 5%\n",
    "+ Class imbalance management - 5%\n",
    "+ Neat and understandable code, with some titles and comments - 0%\n",
    "+ Improved methods from what we discussed in class (properly explained/justified) - 0%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62352bf",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793b7a6d",
   "metadata": {},
   "source": [
    "## Import packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20fb4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from datetime import datetime \n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
    "from sklearn.experimental import enable_halving_search_cv \n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from category_encoders import TargetEncoder, BinaryEncoder, WOEEncoder\n",
    "from sklearn.svm import SVC\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f914fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions  \n",
    "\n",
    "def plot_confusion_matrix(cm, class_labels):\n",
    "    \"\"\"Pretty prints a confusion matrix as a figure\n",
    "\n",
    "    Args:\n",
    "        cm:  A confusion matrix for example\n",
    "        [[245, 5 ], \n",
    "         [ 34, 245]]\n",
    "         \n",
    "        class_labels: The list of class labels to be plotted on x-y axis\n",
    "\n",
    "    Rerturns:\n",
    "        Just plots the confusion matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_cm = pd.DataFrame(cm, index = [i for i in class_labels],\n",
    "                  columns = [i for i in class_labels])\n",
    "    sns.set(font_scale=1)\n",
    "    sns.heatmap(df_cm, annot=True, fmt='g', cmap='Blues')\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"Real label\")\n",
    "    plt.show()\n",
    "\n",
    "def getting_dates(DOBs, ATs):\n",
    "  admit_dates = []\n",
    "  birthdays = []\n",
    "  for i in range(0, len(DOBs)):\n",
    "      birthdays.append(datetime.strptime(DOBs[i], '%Y-%m-%d %H:%M:%S'))\n",
    "  for j in range(0, len(ATs)):\n",
    "      admit_dates.append(datetime.strptime(ATs[j], '%Y-%m-%d %H:%M:%S'))\n",
    "  return birthdays, admit_dates\n",
    "\n",
    "def getting_age(birthdays, admit_dates):\n",
    "  ages = []\n",
    "  for i in range(0, len(birthdays)):\n",
    "    ages.append(((admit_dates[i] - birthdays[i]).days)/365.25)\n",
    "  return ages\n",
    "\n",
    "def age_fix(data, feature):\n",
    "    for i in range(0, len(data[feature])):\n",
    "        if data.loc[i, feature] > 120:\n",
    "            data.loc[i, feature] = 95\n",
    "    return data\n",
    "\n",
    "def drop_feature(features, data):\n",
    "    for feature in features:\n",
    "        data = data.drop(feature, axis = 1)\n",
    "    return data \n",
    "\n",
    "def one_hot_encode(features, data):\n",
    "    for feature in features:\n",
    "        data = pd.get_dummies(data, prefix=[feature], columns=[feature], drop_first = True)\n",
    "    return data\n",
    "\n",
    "def replace(data, feature_to_replace, feature_replacements, new_feature):\n",
    "    data[feature_to_replace] = data[feature_to_replace].replace(feature_replacements, new_feature)\n",
    "    return data\n",
    "\n",
    "def reweight_binary(pi,q1=0.5,r1=0.5):\n",
    "    r0 = 1-r1\n",
    "    q0 = 1-q1\n",
    "    tot = pi*(q1/r1)+(1-pi)*(q0/r0)\n",
    "    w = pi*(q1/r1)\n",
    "    w /= tot\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064df41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup and load data\n",
    "\n",
    "data_path = '/Users/benseimon/Documents/Barca GSE/Studies/Term 2/CML2/Project 1/Data'\n",
    "kaggle_path = '/Users/benseimon/Documents/Barca GSE/Studies/Term 2/CML2/Project 1/Kaggle submissions'\n",
    "model_path = '/Users/benseimon/Documents/Barca GSE/Studies/Term 2/CML2/Project 1/Models'\n",
    "os.chdir(data_path)\n",
    "comorbidities = pd.read_csv('MIMIC_diagnoses.csv')\n",
    "diagnosis_definitions = pd.read_csv('MIMIC_metadata_diagnose.csv')\n",
    "feature_definitions = pd.read_excel('mimic_patient_metadata.xlsx')\n",
    "train_data = pd.read_csv('mimic_train.csv')\n",
    "test_data = pd.read_csv('mimic_test_death.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bdde01",
   "metadata": {},
   "source": [
    "# Step 1: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0171dcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop as per instructions - note that offending columns are not in test set so no need to drop \n",
    "features_to_drop = ['DOD', 'DISCHTIME', 'DEATHTIME', 'LOS', 'Diff']\n",
    "train_data = train_data.drop(features_to_drop, axis=1)\n",
    "test_data = test_data.drop('Diff', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f953c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5428b677",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a2bd5f",
   "metadata": {},
   "source": [
    "## 1a: Numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392d62f6",
   "metadata": {},
   "source": [
    "### 1ai: Generate age variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86be07d1",
   "metadata": {},
   "source": [
    "Using 'DOB' and 'ADMITTIME' we create an age variable. Note that using 'Diff is not necessary since you would simply apply diff to both columns. \n",
    "\n",
    "According to this source (https://github.com/MIT-LCP/mimic-code/issues/637) the data anonymisation process assigns anyone older than 89 to an age close to 300. As a result, I replace any realistic ages to age of 95 (arbitrary). \n",
    "\n",
    "Note that purpose of creating the variable is driven by a simple hypothesis that old age increases the likelihood of dying. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371f3315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create age variable for training set\n",
    "DoB_train, admit_date_train = getting_dates(train_data['DOB'], train_data['ADMITTIME'])\n",
    "ages = getting_age(DoB_train, admit_date_train)\n",
    "train_data['AGE'] = ages\n",
    "print(\"Distribution before amending those with an unrealistic age\")\n",
    "print(train_data['AGE'].describe())\n",
    "train_data = age_fix(train_data, 'AGE')\n",
    "train_data = drop_feature(['DOB', 'ADMITTIME'], train_data)\n",
    "print(\"Distribution after amending those with an unrealistic age\")\n",
    "print(train_data['AGE'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170469be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create age variable for test set\n",
    "DOB_test, admit_date_test = getting_dates(test_data['DOB'], test_data['ADMITTIME'])\n",
    "ages = getting_age(DOB_test, admit_date_test)\n",
    "test_data['AGE'] = ages\n",
    "print(\"Distribution before amending those with an unrealistic age\")\n",
    "print(test_data['AGE'].describe())\n",
    "test_data = age_fix(test_data, 'AGE')\n",
    "test_data = drop_feature(['DOB', 'ADMITTIME'], test_data)\n",
    "print(\"Distribution after amending those with an unrealistic age\")\n",
    "print(test_data['AGE'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37ce15e",
   "metadata": {},
   "source": [
    "### 1aii: Investigate relationship between outcome and numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fa089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign features\n",
    "identifiers = ['subject_id', 'hadm_id', 'icustay_id']\n",
    "numerical_features = ['HeartRate_Min', 'HeartRate_Max', 'HeartRate_Mean', 'SysBP_Min',\n",
    "       'SysBP_Max', 'SysBP_Mean', 'DiasBP_Min', 'DiasBP_Max', 'DiasBP_Mean',\n",
    "       'MeanBP_Min', 'MeanBP_Max', 'MeanBP_Mean', 'RespRate_Min',\n",
    "       'RespRate_Max', 'RespRate_Mean', 'TempC_Min', 'TempC_Max', 'TempC_Mean',\n",
    "       'SpO2_Min', 'SpO2_Max', 'SpO2_Mean', 'Glucose_Min', 'Glucose_Max',\n",
    "       'Glucose_Mean', 'AGE']\n",
    "categorical_features = ['GENDER', 'ADMISSION_TYPE', 'INSURANCE', 'RELIGION',\n",
    "       'MARITAL_STATUS', 'ETHNICITY', 'FIRST_CAREUNIT']\n",
    "diagnosis_features = ['DIAGNOSIS','ICD9_diagnosis']\n",
    "target = ['HOSPITAL_EXPIRE_FLAG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd771306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check\n",
    "len(identifiers+numerical_features+categorical_features+diagnosis_features+target) == train_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a659b1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_numerical = train_data[numerical_features+target].corr()\n",
    "sns.set(rc = {'figure.figsize':(20,10)})\n",
    "sns.heatmap(corr_numerical, cmap = 'RdBu_r', linecolor = 'white', vmin=-1, vmax=1, annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f3509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_numerical['HOSPITAL_EXPIRE_FLAG'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbeb112",
   "metadata": {},
   "source": [
    "Clear positive correlation for RespRate_Mean, RespRate_Max, HeartRate_Max, HeartRate_Mean, Glucose_Mean and our new variable AGE\n",
    "\n",
    "Clear negative correlation for DiasBP_Mean, TempC_Mean, MeanBP_Mean, SysBP_Mean, TempC_Min, DiasBP_Min,           SpO2_Mean, MeanBP_Min, SysBP_Min, SpO2_Min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846e7cee",
   "metadata": {},
   "source": [
    "## Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcb192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[categorical_features].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0d0a1e",
   "metadata": {},
   "source": [
    "Ethnicity and religion have a higher number of unique categories. Let's address this to reduce dimensionality. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d42156",
   "metadata": {},
   "source": [
    "### Ethnicities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a6312",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['ETHNICITY'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26eaf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorise ethnicities \n",
    "NATIVE = ['AMERICAN INDIAN/ALASKA NATIVE',\n",
    " 'AMERICAN INDIAN/ALASKA NATIVE FEDERALLY RECOGNIZED TRIBE',  'NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER']\n",
    "ASIAN = ['ASIAN',\n",
    " 'ASIAN - ASIAN INDIAN',\n",
    " 'ASIAN - CAMBODIAN',\n",
    " 'ASIAN - CHINESE',\n",
    " 'ASIAN - FILIPINO',\n",
    " 'ASIAN - JAPANESE',\n",
    " 'ASIAN - KOREAN',\n",
    " 'ASIAN - OTHER',\n",
    " 'ASIAN - THAI',\n",
    " 'ASIAN - VIETNAMESE']\n",
    "BLACK = ['BLACK/AFRICAN',\n",
    " 'BLACK/AFRICAN AMERICAN',\n",
    " 'BLACK/CAPE VERDEAN',\n",
    " 'BLACK/HAITIAN']\n",
    "OTHER = ['CARIBBEAN ISLAND', 'OTHER', 'MIDDLE EASTERN','MULTI RACE ETHNICITY']\n",
    "\n",
    "HISPANIC_LATINO = ['HISPANIC OR LATINO',\n",
    " 'HISPANIC/LATINO - CENTRAL AMERICAN (OTHER)',\n",
    " 'HISPANIC/LATINO - COLOMBIAN',\n",
    " 'HISPANIC/LATINO - CUBAN',\n",
    " 'HISPANIC/LATINO - DOMINICAN',\n",
    " 'HISPANIC/LATINO - GUATEMALAN',\n",
    " 'HISPANIC/LATINO - HONDURAN',\n",
    " 'HISPANIC/LATINO - MEXICAN',\n",
    " 'HISPANIC/LATINO - PUERTO RICAN',\n",
    " 'HISPANIC/LATINO - SALVADORAN', \n",
    "                   'SOUTH AMERICAN']\n",
    "UNKNOWN = ['UNABLE TO OBTAIN',\n",
    " 'UNKNOWN/NOT SPECIFIED']\n",
    "WHITE = ['WHITE',\n",
    " 'WHITE - BRAZILIAN',\n",
    " 'WHITE - EASTERN EUROPEAN',\n",
    " 'WHITE - OTHER EUROPEAN',\n",
    " 'WHITE - RUSSIAN',\n",
    "        'PORTUGUESE']\n",
    "replacing_ethnicities = [NATIVE, ASIAN, BLACK, OTHER, HISPANIC_LATINO, UNKNOWN, WHITE]\n",
    "replacing_ethnicities_str = ['NATIVE', 'ASIAN', 'BLACK', 'OTHER', 'HISPANIC_LATINO', 'UNKNOWN', 'WHITE']\n",
    "\n",
    "#'PATIENT DECLINED TO ANSWER' are left as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fb4a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply categorisation of ethnicities to dataframe\n",
    "counter = 0\n",
    "for i in replacing_ethnicities:\n",
    "    train_data = replace(train_data, 'ETHNICITY', i, replacing_ethnicities_str[counter])\n",
    "    test_data = replace(test_data, 'ETHNICITY', i, replacing_ethnicities_str[counter])\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c007679",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['ETHNICITY'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2667f4fa",
   "metadata": {},
   "source": [
    "### Religion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0444ced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['RELIGION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea9ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crude categorisation of religions to reduce dimensionality \n",
    "religion_other = ['HEBREW', 'UNITARIAN-UNIVERSALIST', 'HINDU', 'GREEK ORTHODOX',\"JEHOVAH'S WITNESS\", \"BUDDHIST\", 'MUSLIM', 'OTHER', 'CHRISTIAN SCIENTIST', 'EPISCOPALIAN', 'ROMANIAN EAST. ORTH', '7TH DAY ADVENTIST'] \n",
    "train_data = replace(train_data, 'RELIGION', religion_other, 'OTHER')\n",
    "test_data = replace(test_data, 'RELIGION', religion_other, 'OTHER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a56f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['RELIGION'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b10ff2",
   "metadata": {},
   "source": [
    "### Investigate relationship between outcome and categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4099c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dfs = []\n",
    "categorical_check = pd.DataFrame(columns = ['HOSPITAL_EXPIRE_FLAG', 'Sub-category', 'Counts', 'Sum', '%', 'Variable'])\n",
    "for i in categorical_features:\n",
    "    new_col_count = str(i+'_count')\n",
    "    new_col_sum = str(i+'sum')\n",
    "    counts = train_data.groupby('HOSPITAL_EXPIRE_FLAG').agg({i: 'value_counts'}).rename(columns = {i: new_col_count}).reset_index()\n",
    "    totals = counts.groupby(i).agg({new_col_count: sum}).rename(columns = {new_col_count: new_col_sum}).reset_index()\n",
    "    counts = pd.merge(counts, totals, how ='left', on=i)\n",
    "    counts['%'] = counts[new_col_count]/counts[new_col_sum]\n",
    "    counts['Variable'] = i\n",
    "    counts.columns = categorical_check.columns\n",
    "    categorical_check = pd.concat([categorical_check, counts], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ead263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in categorical_features:\n",
    "    print(categorical_check[(categorical_check['HOSPITAL_EXPIRE_FLAG'] == 1) & (categorical_check['Variable'] == i)].sort_values(by='%', ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4eee87",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "- Limited/no variation by type: Gender\n",
    "- Admission_type: Emergency = higher likelihood of death\n",
    "- Insurance: Self-pay/medicare = higher likelihood of death\n",
    "- Religion, marital_status, ethnicity: unknown categorisation = higher likelihood of death\n",
    "- First care unit: CSRU = lower likelihood of death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef2310e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[categorical_features].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1328417",
   "metadata": {},
   "source": [
    "Have managed to reduce dimensionality so will one_hot_encode for all features here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192cc95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_check[(categorical_check['HOSPITAL_EXPIRE_FLAG'] == 1)].sort_values(by='%', ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53f0565",
   "metadata": {},
   "source": [
    "This is weird and I cannot think of an obviously good imputation method, so I drop. One cool idea (I wasn't sure how to execute) \n",
    "\n",
    "This provides a good justification for trying a range of encoders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be12fdef",
   "metadata": {},
   "source": [
    "To note that we may have some data missing at ... since unknown ethnicity/religion/marital status have high proportions of death. I choose to actually turn these into an NA, and then iteratively impute them using on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca38cbd8",
   "metadata": {},
   "source": [
    "# Step 2: Dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0134ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#report nas\n",
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a38b0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#report nas\n",
    "test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754c0a78",
   "metadata": {},
   "source": [
    "## Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f585fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[numerical_features].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf68598",
   "metadata": {},
   "source": [
    "Given the high correlations between groups of variables, the first impression is that imputation could be helpful. For example, imagine we had heart rate min and max, but not mean. A simple regression may help us accurately impute the mean. The same applies across all variables, and then in combination with the iterative imputer we could hopefully produce some reasonable imputations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e37a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group\n",
    "heart_features = ['HeartRate_Min', 'HeartRate_Max', 'HeartRate_Mean']\n",
    "bp_features = ['SysBP_Min','SysBP_Max', 'SysBP_Mean', 'DiasBP_Min', 'DiasBP_Max', 'DiasBP_Mean','MeanBP_Min', 'MeanBP_Max', 'MeanBP_Mean']\n",
    "resp_features = ['RespRate_Min','RespRate_Max', 'RespRate_Mean']\n",
    "temp_features = ['TempC_Min', 'TempC_Max', 'TempC_Mean']\n",
    "oxygen_features = ['SpO2_Min', 'SpO2_Max', 'SpO2_Mean']\n",
    "glucose_features = ['Glucose_Min', 'Glucose_Max','Glucose_Mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d92d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[heart_features+bp_features+resp_features+temp_features+oxygen_features+glucose_features].isnull().sum(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b2b7bb",
   "metadata": {},
   "source": [
    "However, here we see that for those observations with missing values, a high proportion are actually missing data across all numerical features. The exception seems to be the glucose variables since there are far fewer missing values. \n",
    "\n",
    "This has implications for our imputation strategy. This provides a motivation to use the iterative imputer, using the glucose features as the starting point. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c2b38",
   "metadata": {},
   "source": [
    "## Marital status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1800cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "marital_unknown = train_data['MARITAL_STATUS'] == 'UNKNOWN (DEFAULT)'\n",
    "ethnicity_unknown = train_data['ETHNICITY'] == 'UNKNOWN'\n",
    "religion_unknown = train_data['RELIGION'] == 'UNOBTAINABLE'\n",
    "religion_unspecified = train_data['RELIGION'] == 'NOT SPECIFIED'\n",
    "marital_na = train_data['MARITAL_STATUS'].isna()\n",
    "\n",
    "print(len(train_data[marital_na]))\n",
    "print(len(train_data[marital_na&(religion_unknown|ethnicity_unknown)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679fe184",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[marital_na]['HOSPITAL_EXPIRE_FLAG'].sum()/len(train_data[marital_na])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb0018b",
   "metadata": {},
   "source": [
    "Implications for imputation method. A simple method would be to impute the modal category but this doesn't seem sensible here given that a) high proportion of deaths for those with na and b) we notice those with an NA for marital status also tend to have classified relgion/ethnicity as unknown. \n",
    "\n",
    "As a result I classify these individuals into their own category - \"Missing\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bbe67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['MARITAL_STATUS'].fillna('MISSING', inplace = True)\n",
    "test_data['MARITAL_STATUS'].fillna('MISSING', inplace = True)\n",
    "train_data['MARITAL_STATUS'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6055c039",
   "metadata": {},
   "source": [
    "# Step 3: Generate new features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0973dd6c",
   "metadata": {},
   "source": [
    "## 3a: Comorbidities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dac08f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "comorbidities.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d18d8c8",
   "metadata": {},
   "source": [
    "In the the comorbidities data we have some nas for SEQ_NUM (refers to number of additional comorbidities diagnosed for a given HADM_ID), and ICD9_CODE. Let's investigate a bit further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaf80d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comorbidities[comorbidities['SEQ_NUM'].isnull() == True])\n",
    "print(comorbidities[comorbidities['SEQ_NUM'].isnull() == True].shape)\n",
    "comorbidities.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd405fc",
   "metadata": {},
   "source": [
    "So we have 47 rows with nas out of 651,047 which is very few. In any case, let's take the first subject_id as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c92b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "comorbidities[comorbidities['SUBJECT_ID'] == 9998]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66af20b",
   "metadata": {},
   "source": [
    "In other words, the nas represent individuals who have gone to hospital but have not received a diagnosis. However, this doesn't raise an issue since we will only be considering the no of comorbidities for each hospital admission. We see this later when applying the group by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c13654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use group by to get the no. of comorbitidies per hospital admission\n",
    "comorbidities_count = comorbidities.groupby(['SUBJECT_ID','HADM_ID'], as_index = False).agg({'ICD9_CODE': 'nunique'})\n",
    "#rename columns for readability \n",
    "comorbidities_count.rename(columns={'ICD9_CODE': 'No_comorbs'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b8e1e9",
   "metadata": {},
   "source": [
    "Below shows that we capture the correct number of diagnoses per hospital admission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3d2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirm no need to remove nas\n",
    "comorbidities_count[comorbidities_count['SUBJECT_ID']==9998]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecf7ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with dataset\n",
    "comorbidities_count.columns= comorbidities_count.columns.str.lower()\n",
    "train_data = train_data.merge(comorbidities_count[['subject_id', 'hadm_id', 'no_comorbs']], on=['subject_id', 'hadm_id'])\n",
    "test_data = test_data.merge(comorbidities_count[['subject_id', 'hadm_id', 'no_comorbs']], on=['subject_id', 'hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8a6249",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f89d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = numerical_features+['no_comorbs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49b7e29",
   "metadata": {},
   "source": [
    "## 3b: Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a211b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_survived = train_data['HOSPITAL_EXPIRE_FLAG'].value_counts()[0]\n",
    "total_died = train_data['HOSPITAL_EXPIRE_FLAG'].value_counts()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5697cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_diagnosed = train_data['ICD9_diagnosis'].value_counts().reset_index().rename(columns = {'ICD9_diagnosis': 'no_diagnosed', 'index': 'ICD9_diagnosis'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c32e122",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_diagnosis = train_data.groupby('ICD9_diagnosis').agg({'HOSPITAL_EXPIRE_FLAG': 'sum'}).sort_values(by = 'HOSPITAL_EXPIRE_FLAG', ascending = False).rename(columns = {'HOSPITAL_EXPIRE_FLAG': 'died'}).reset_index()\n",
    "by_diagnosis = pd.merge(by_diagnosis, total_diagnosed, how = 'left', on ='ICD9_diagnosis')\n",
    "by_diagnosis['survived'] = by_diagnosis['no_diagnosed'] -  by_diagnosis['died']\n",
    "by_diagnosis['%_of_total_dead'] = by_diagnosis['died']/total_died\n",
    "by_diagnosis['%_of_diagnosed_dead'] = by_diagnosis['died']/by_diagnosis['no_diagnosed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca39f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_diagnosis.sort_values(by = '%_of_total_dead', ascending = False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95683522",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dead = 10\n",
    "condition1 = by_diagnosis['died']>min_dead\n",
    "print(by_diagnosis[by_diagnosis['died']>min_dead].sort_values(by = '%_of_diagnosed_dead', ascending = False).shape)\n",
    "by_diagnosis[condition1].sort_values(by = '%_of_diagnosed_dead', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4aa477",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(by_diagnosis[by_diagnosis['died']>5].sort_values(by = '%_of_diagnosed_dead', ascending = False)['%_of_diagnosed_dead'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58da8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "anyone_died = list(by_diagnosis[by_diagnosis['died']>0]['ICD9_diagnosis'].unique())\n",
    "percentage_total_dead = by_diagnosis[['ICD9_diagnosis', '%_of_total_dead']]\n",
    "percentage_deadly_diagnosis = by_diagnosis[condition1][['ICD9_diagnosis', '%_of_diagnosed_dead']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3e9260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include % total dead as a score\n",
    "train_data = pd.merge(train_data, percentage_total_dead, on = 'ICD9_diagnosis', how = 'left')\n",
    "train_data = pd.merge(train_data, percentage_deadly_diagnosis, on = 'ICD9_diagnosis', how = 'left')\n",
    "#give a 0 if nobody diagnosed with disease died\n",
    "train_data['%_of_diagnosed_dead'].fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b69547",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['anyone_died'] = pd.Series(dtype = object)\n",
    "for i in range(0, len(train_data['ICD9_diagnosis'])):\n",
    "    if train_data['ICD9_diagnosis'][i] in anyone_died:\n",
    "        train_data.at[i, 'anyone_died'] = 'Someone died'\n",
    "    else:\n",
    "        train_data.at[i, 'anyone_died'] = 'Noone died'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc240867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include % total dead as a score\n",
    "test_data = pd.merge(test_data, percentage_total_dead, on = 'ICD9_diagnosis', how = 'left')\n",
    "test_data = pd.merge(test_data, percentage_deadly_diagnosis, on = 'ICD9_diagnosis', how = 'left')\n",
    "#give a 0 if nobody diagnosed with disease died\n",
    "test_data['%_of_diagnosed_dead'].fillna(0, inplace=True)\n",
    "test_data['anyone_died'] = pd.Series(dtype = object)\n",
    "#for loop\n",
    "for i in range(0, len(test_data['ICD9_diagnosis'])):\n",
    "    if test_data['ICD9_diagnosis'][i] in anyone_died:\n",
    "        test_data.at[i, 'anyone_died'] = 'Someone died'\n",
    "    else:\n",
    "        test_data.at[i, 'anyone_died'] = 'Noone died'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca09ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "added_numerical_features = ['%_of_total_dead', '%_of_diagnosed_dead']\n",
    "added_categorical_features = ['anyone_died']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd18b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = numerical_features + added_numerical_features\n",
    "categorical_features = categorical_features + added_categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad21217e",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce8959",
   "metadata": {},
   "source": [
    "### Pipeline and fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d263f0",
   "metadata": {},
   "source": [
    "numerical_features\n",
    "numerical_transformer_iterative = Pipeline(\n",
    "    steps=[(\"imputer\", IterativeImputer(random_state=0, missing_values = np.nan, initial_strategy = 'mean', max_iter=30,imputation_order = 'ascending', add_indicator=True)), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "#see below for simple imputer which was also tried with strategy = 'mean' and strategy = 'median'\n",
    "#numerical_transformer_simple = Pipeline(\n",
    "    #steps=[(\"imputer\", SimpleImputer(missing_values = np.nan, strategy = 'mean', add_indicator=True)), (\"scaler\", StandardScaler())])\n",
    "\n",
    "\n",
    "categorical_features\n",
    "categorical_transformer_one_hot = OneHotEncoder(drop='if_binary',handle_unknown=\"ignore\")\n",
    "categorical_transformer_target = TargetEncoder() #target encoder\n",
    "categorical_transformer_WOE = WOEEncoder(regularization = 0.2, randomized=True) #weight of evidence encoder\n",
    "#set handle_unknown to ignore, since if it encounters an unseen categorical type in the test set, it will automatically create a column of 0s \n",
    "\n",
    "diagnosis_features = ['ICD9_diagnosis']\n",
    "diagnosis_transformer_binary = BinaryEncoder()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numerical_transformer_iterative, numerical_features),\n",
    "        (\"cat\", categorical_transformer_target, categorical_features),\n",
    "        #(\"diag\", diagnosis_transformer_binary, diagnosis_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "knn_estimator = KNeighborsClassifier(algorithm='auto', weights='distance') \n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "knn_pipeline = imbPipeline([(\"preprocessor\", preprocessor), ('sampling', SMOTE()), (\"classifier\", knn_estimator)]\n",
    ")\n",
    "\n",
    "grid_values = [{'classifier__n_neighbors':[50,100,200,300,400, 500],\n",
    "               #'classifier__weights':['uniform', 'distance'], \n",
    "                'classifier__p':[1, 2]}] #distance consistently outperforms uniform so commented out\n",
    "grid_knn = HalvingGridSearchCV(knn_pipeline, param_grid = grid_values, scoring = 'roc_auc', cv = 5, verbose = 3) #used to narrow down range for full GridSearch. First run used \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f7fa5d",
   "metadata": {},
   "source": [
    "grid_knn.fit(train_data[numerical_features+categorical_features+diagnosis_features], train_data[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dfe784",
   "metadata": {},
   "source": [
    "# save model\n",
    "#filename = 'Numerical_iterativeimp__categorical__onehotenc.sav'\n",
    "filename = 'Numerical_iterativeimp__categorical__targetenc.sav'\n",
    "pickle.dump(grid_knn, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf11e1d",
   "metadata": {},
   "source": [
    "print('best parameters:', grid_knn.best_params_)\n",
    "print('best score:', grid_knn.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f9f310",
   "metadata": {},
   "source": [
    "### Results and reweight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed108250",
   "metadata": {},
   "source": [
    "#get probabilities \n",
    "# load the model from disk and use it\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "knn_train_probs = loaded_model.predict_proba(train_data)\n",
    "knn_test_probs = loaded_model.predict_proba(test_data)\n",
    "knn_probs = {'train_probs': knn_train_probs, 'test_probs': knn_test_probs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8302fb9",
   "metadata": {},
   "source": [
    "knn_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49b8ced",
   "metadata": {},
   "source": [
    "q1 = train_data[target].sum()[0]/len(train_data[target])\n",
    "r1 = 0.5\n",
    "for key, value in knn_probs.items():\n",
    "    if len(np.unique(knn_probs[key])) > 2:\n",
    "        #necessary because sometimes we only get probabilities of 1 and 0 \n",
    "        knn_probs[key] = pd.DataFrame(knn_probs[key]).apply(reweight_binary,args=(q1,r1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f3f08d",
   "metadata": {},
   "source": [
    "#plot confusion matrix \n",
    "class_labels = [\"Survived\",\"Died\"]\n",
    "labels = [0,1]\n",
    "cm = confusion_matrix(y_pred=knn_probs['train_probs'][:, 1], y_true=train_data[target], labels=labels)\n",
    "plot_confusion_matrix(cm, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8ca1f9",
   "metadata": {},
   "source": [
    "Definitely overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2b8019",
   "metadata": {},
   "source": [
    "# Produce .csv for kaggle testing \n",
    "os.chdir(kaggle_path)\n",
    "test_predictions_submit = pd.DataFrame({\"icustay_id\": test_data[\"icustay_id\"], \"HOSPITAL_EXPIRE_FLAG\": knn_probs['test_probs'][1]})\n",
    "test_predictions_submit.to_csv(\"test_predictions_KNN_submit.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68349a62",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd73f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_transformer_iterative = Pipeline(\n",
    "    steps=[(\"imputer\", IterativeImputer(\n",
    "random_state=0, missing_values = np.nan, initial_strategy = 'mean', max_iter=30,imputation_order = 'ascending', add_indicator=True)), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "#KNNImputer(missing_values=np.nan, n_neighbors = 100, weights = 'distance', add_indicator = False)\n",
    "#(estimator = KNeighborsRegressor(n_neighbors=50, weights='distance', algorithm = 'auto'),\n",
    "#see below for simple imputer which was also tried with strategy = 'mean' and strategy = 'median'\n",
    "#numerical_transformer_simple = Pipeline(\n",
    "    #steps=[(\"imputer\", SimpleImputer(missing_values = np.nan, strategy = 'mean', add_indicator=True)), (\"scaler\", StandardScaler())])\n",
    "\n",
    "\n",
    "categorical_transformer_one_hot = OneHotEncoder(drop='if_binary',handle_unknown=\"ignore\")\n",
    "categorical_transformer_target = TargetEncoder() #target encoder\n",
    "categorical_transformer_WOE = WOEEncoder(regularization = 0.2, randomized=True) #weight of evidence encoder\n",
    "#set handle_unknown to ignore, since if it encounters an unseen categorical type in the test set, it will automatically create a column of 0s \n",
    "\n",
    "diagnosis_features = ['ICD9_diagnosis']\n",
    "diagnosis_transformer_binary = BinaryEncoder()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numerical_transformer_iterative, numerical_features),\n",
    "        (\"cat\", categorical_transformer_target, categorical_features),\n",
    "        #(\"diag\", diagnosis_transformer_binary, diagnosis_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "svm_estimator = SVC(probability = True, class_weight = 'balanced', random_state = 5) #class_weight set to balanced since we have an unbalanced dataset. Could have grid searched for none vs balanced, but did not due to time constraints. \n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "svm_pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", svm_estimator)])\n",
    "\n",
    "#'classifier__C':[0.01, 0.1, 1, 10]\n",
    "#'classifier__gamma':[0.01, 0.1, 0.3], \n",
    "grid_values = [{'classifier__kernel': ['rbf'],'classifier__C':[0.1, 1, 2],\n",
    "               'classifier__gamma':[0.005, 0.01, 0.02, 0.05, 0.075], }]\n",
    "grid_svm = HalvingGridSearchCV(svm_pipeline, param_grid = grid_values, scoring = 'roc_auc', cv = 3, verbose = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a72873",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_svm.fit(train_data[numerical_features+categorical_features], train_data[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae60492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "#filename = 'Numerical_iterativeimp__categorical__onehotenc.sav'\n",
    "filename = 'Numerical_iterativeimp__categorical_targetenc__addedfeatures1__gridgamma__SVM.sav'\n",
    "pickle.dump(grid_svm, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4547c0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best parameters:', grid_svm.best_params_)\n",
    "print('best score:', grid_svm.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3b0524",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Results and reweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a9cc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get probabilities \n",
    "# load the model from disk and use it\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "svm_train_probs = loaded_model.predict_proba(train_data)\n",
    "svm_test_probs = loaded_model.predict_proba(test_data)\n",
    "svm_probs = {'train_probs': svm_train_probs, 'test_probs': svm_test_probs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90ab542",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2f0043",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = train_data[target].sum()[0]/len(train_data[target])\n",
    "r1 = 0.5\n",
    "for key, value in svm_probs.items():\n",
    "    if len(np.unique(svm_probs[key])) > 2:\n",
    "        #necessary because sometimes we only get probabilities of 1 and 0 \n",
    "        svm_probs[key] = pd.DataFrame(svm_probs[key]).apply(reweight_binary,args=(q1,r1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14349a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_probs['train_probs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d7e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusion matrix \n",
    "class_labels = [\"Survived\",\"Died\"]\n",
    "labels = [0,1]\n",
    "cm = confusion_matrix(y_pred=svm_train_probs[:, 1], y_true=train_data[target], labels=labels)\n",
    "plot_confusion_matrix(cm, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f723e8",
   "metadata": {},
   "source": [
    "Definitely overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195628ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce .csv for kaggle testing \n",
    "os.chdir(kaggle_path)\n",
    "test_predictions_submit = pd.DataFrame({\"icustay_id\": test_data[\"icustay_id\"], \"HOSPITAL_EXPIRE_FLAG\": svm_test_probs[:,1]})\n",
    "test_predictions_submit.to_csv(\"test_predictions_svm_submit_added_features1_gridgamma.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464cf4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d33b8dfb",
   "metadata": {},
   "source": [
    "## Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da37e204",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Preprocessing \n",
    "\n",
    "##a\n",
    "\n",
    "###i - make dummies for categorical features\n",
    "\n",
    "for_dummy = categorical_features\n",
    "for_imputation = numerical_features\n",
    "\n",
    "#dummy = pd.get_dummies(train_data[categorical_features], drop_first = True)\n",
    "dummy = make_column_transformer((OneHotEncoder(drop = 'if_binary', handle_unknown = 'ignore'), for_dummy), remainder = 'passthrough') #set handle_unknown to ignore, since if it encounters an unseen categorical type in the test set, it will automatically create a column of 0s \n",
    "\n",
    "###ii - iterative imputer for numerical features\n",
    "imputer = make_column_transformer((IterativeImputer(random_state=0, missing_values = np.nan, initial_strategy = 'mean', max_iter=30,imputation_order = 'ascending', add_indicator=True), for_imputation), (StandardScaler(), for_imputation), remainder = 'passthrough') #ascending i.e. start with columns with fewest missing values \n",
    "#imputer = make_column_transformer((SimpleImputer(missing_values = np.nan, strategy = 'mean', add_indicator=True), for_imputation), (StandardScaler(), for_imputation), remainder = 'passthrough') #ascending i.e. start with columns with fewest missing values \n",
    "\n",
    "#2 estimator \n",
    "knn_estimator = KNeighborsClassifier(algorithm='auto', weights='distance') \n",
    "#choose not to grid search over different algorithms since sklearn selects the most appropriate, but am aware there are multiple options. \n",
    "\n",
    "#3 pipeline\n",
    "knn_pipeline = imbPipeline([#('dummy', dummy), \n",
    "                    ('imputer', imputer),\n",
    "                    ('scale', StandardScaler())\n",
    "                            ('sampling', SMOTE()),\n",
    "                            ('classifier', knn_estimator)])\n",
    "\n",
    "grid_values = [{'classifier__n_neighbors':[1]#,10,50,100,200, 500],\n",
    "               ,'classifier__weights':['uniform', 'distance'], 'classifier__p':[1]}]\n",
    "grid_knn = GridSearchCV(knn_pipeline, param_grid = grid_values, scoring = 'roc_auc', cv = 5, verbose = 3)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031cb741",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_knn.fit(train_data_check.drop(target, axis=1), train_data[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a008a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imputer__estimator = BayesianRidge(),\n",
    "    DecisionTreeRegressor(max_features='sqrt', random_state=0),\n",
    "    ExtraTreesRegressor(n_estimators=20, random_state=0),\n",
    "    KNeighborsRegressor(n_neighbors=2)\n",
    "\n",
    "\n",
    "numeric_transformer_iterative = Pipeline(\n",
    "    steps=[(\"imputer\", (random_state=0, estimator=my_estimator, max_iter=30,add_indicator=True)), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "numeric_transformer_simple = Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"mean\", add_indicator = True)), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "categorical_features = \n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "knn_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),])\n",
    "\n",
    "#make dummies\n",
    "train_data = one_hot_encode(['ETHNICITY'], train_data)\n",
    "test_data = one_hot_encode(['ETHNICITY'], test_data)\n",
    "                                                train_data = one_hot_encode(['GENDER', 'ADMISSION_TYPE', 'INSURANCE', 'MARITAL_STATUS',\n",
    "                               'FIRST_CAREUNIT', 'RELIGION'], train_data)\n",
    "test_data = one_hot_encode(['GENDER', 'ADMISSION_TYPE', 'INSURANCE', 'MARITAL_STATUS',\n",
    "                               'FIRST_CAREUNIT', 'RELIGION'], test_data)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "                                                \n",
    "                                                \n",
    "#2 Imputer \n",
    "\n",
    "#3 Estimator \n",
    "\n",
    "\n",
    "#4 Make pipeline \n",
    "\n",
    "pipe_knn = imbPipeline([('dummy', column_dummy), \n",
    "                ('imputer', KNNImputer(missing_values=np.nan, n_neighbors = 100, weights = 'distance', add_indicator = False)),\n",
    "                ('preprocessing', preprocessing.StandardScaler()),\n",
    "                ('sampling', SMOTE()),\n",
    "                #('features', fs.RFECV(estimator = DecisionTreeClassifier(class_weight = 'balanced'),\n",
    "                #                      step = 10, cv = 5, scoring = 'roc_auc', verbose = 0)),\n",
    "                ('classifier', KNeighborsClassifier(n_neighbors = 20,\n",
    "                                                    weights = 'distance',\n",
    "                                                    algorithm = 'auto'))])\n",
    "knn_pipeline = imbPipeline([(\"preprocessor\", knn_preprocessor), (\"classifier\", knn_estimator)])\n",
    "\n",
    "#5 Run GridSearch \n",
    "\n",
    "grid_values = [#{'imputer__n_neighbors':[100]},\n",
    "               {'classifier__n_neighbors':[1,5,10,50,100,200, 500],\n",
    "               'classifier__weights':['uniform', 'distance'], 'classifier__p':[1, 2]}]\n",
    "grid_knn = GridSearchCV(knn_pipeline, param_grid = grid_values, scoring = 'roc_auc', cv = 5, verbose = 3)\n",
    "   \n",
    "\n",
    "    # load the model from disk and use it\n",
    "\n",
    "\n",
    "#6 Train, fit and save model \n",
    "grid_knn.fit(X_train, y_train)\n",
    "filename = 'my_model.sav'\n",
    "pickle.dump(my_model, open(filename, 'wb'))\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "loaded_model.predict_proba(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e95b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in categorical_features:\n",
    "    print(train_data.groupby(i).agg({'HOSPITAL_EXPIRE_FLAG': 'value_counts'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d50b2",
   "metadata": {},
   "source": [
    "So we have some very highly correlated features. With regard to missing values imputation this should prove useful. Simply imputing using the mean/median is a valid approach. However, it is reasonably crude. Take heart rate as an example - not logical to use the mean/median, especially given the high degree of correlation between variables. \n",
    "\n",
    "Note that I assume we are in a 'Missing at Random' world. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e189cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each variable, I select the variables which exhibit obvious correlations and investigate the type of correlation by visual inspection\n",
    "corr[(corr.iloc[:, :]>0.2) | (corr.iloc[:, :] <-0.2)]\n",
    "highly_correlated = {}\n",
    "for i in corr.columns:\n",
    "    correlated_vars = corr[corr[i]>0.2].index\n",
    "    highly_correlated[i] = correlated_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f092310",
   "metadata": {},
   "outputs": [],
   "source": [
    "highly_correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c5f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr[corr['HeartRate_Min']>0.2].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed24828",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train_data[numerical_features+target], hue = 'HOSPITAL_EXPIRE_FLAG', diag_kind = 'kde', palette = 'bright')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eb3434",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data[['HeartRate_Min', 'HeartRate_Max', 'HeartRate_Mean']].isnull().sum(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f697eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[['HeartRate_Min', 'HeartRate_Max', 'HeartRate_Mean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde4d632",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameImputer(TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "\n",
    "        Columns of dtype object are imputed with the most frequent value \n",
    "        in column.\n",
    "\n",
    "        Columns of other types are imputed with mean of column.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"Impute values for categorical features where data was recorded as unknown with other categorical features in dataset\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n",
    "            index=X.columns)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "\n",
    "data = [\n",
    "    ['a', 1, 2],\n",
    "    ['b', 1, 1],\n",
    "    ['b', 2, 2],\n",
    "    [np.nan, np.nan, np.nan]\n",
    "]\n",
    "\n",
    "X = pd.DataFrame(data)\n",
    "xt = DataFrameImputer().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712edd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[['HeartRate_Min', 'HeartRate_Max', 'HeartRate_Mean']].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7032cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[['SysBP_Min','SysBP_Max', 'SysBP_Mean', 'DiasBP_Min', 'DiasBP_Max', 'DiasBP_Mean','MeanBP_Min', 'MeanBP_Max', 'MeanBP_Mean']].isnull().sum(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854ecbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[['RespRate_Min',\n",
    "       'RespRate_Max', 'RespRate_Mean']].isnull().sum(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba9b47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[['TempC_Min', 'TempC_Max', 'TempC_Mean']].isnull().sum(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52db908",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbc5336",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[['SpO2_Min', 'SpO2_Max', 'SpO2_Mean']].isnull().sum(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e411166",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[['Glucose_Min', 'Glucose_Max',\n",
    "       'Glucose_Mean']].isnull().sum(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b34ce04",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data[['HeartRate_Min', 'HeartRate_Max', 'HeartRate_Mean', 'SysBP_Min',\n",
    "       'SysBP_Max', 'SysBP_Mean', 'DiasBP_Min', 'DiasBP_Max', 'DiasBP_Mean',\n",
    "       'MeanBP_Min', 'MeanBP_Max', 'MeanBP_Mean', 'RespRate_Min',\n",
    "       'RespRate_Max', 'RespRate_Mean', 'TempC_Min', 'TempC_Max', 'TempC_Mean',\n",
    "       'SpO2_Min', 'SpO2_Max', 'SpO2_Mean', 'Glucose_Min', 'Glucose_Max',\n",
    "       'Glucose_Mean']].isnull().sum(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6c04a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_cumulative = np.cumsum(train_data['ICD9_diagnosis'].value_counts(normalize=True).sort_values(ascending=False))\n",
    "px.area(\n",
    "x=range(1, diagnosis_cumulative.shape[0]+1), \n",
    "y = diagnosis_cumulative,\n",
    "labels={\"x\": \"diagnosis\", \"y\": \"Proportion of patients\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8547f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329c66fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "comorbidities.merge(by_diagnosis[['ICD9_diagnosis', '%_of_total_dead', '%_of_diagnosed_dead']], on = ['ICD9_diagnosis'], how='left')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
