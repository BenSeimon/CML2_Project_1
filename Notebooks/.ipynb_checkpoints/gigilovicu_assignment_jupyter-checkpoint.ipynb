{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbours and Support Vector Classifiers \n",
    "## Gian-Piero Lovicu\n",
    "This notebook contains commentary and code to run a K-Nearest Neighbours (KNN) and Support Vector Classifier (SVC) models on the mimic dataset. The notebook is structured as follows:\n",
    "\n",
    "* Data cleaning and feature creation, which applies to both models\n",
    "* Fit KNN classifier to the data\n",
    "* Fit a SVC to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Feature Creation\n",
    "## Step 0: Import packages\n",
    "\n",
    "Specific functions are imported when they are called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages and functions\n",
    "#Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "#sklearn functions\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn import preprocessing\n",
    "from imblearn.over_sampling import SMOTENC, SMOTE, RandomOverSampler\n",
    "import sklearn.feature_selection as fs\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import data\n",
    "Import training and test sets, detailed diagnoses data and custom ethnicity mapping (I manually mapped ethnicity to a reduced set of categories for ease of interpretation). The ethnicity mapping file was submitted with this notebook.\n",
    "\n",
    "Note - you may need to change path to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/gigilovicu/Documents/masters/semester_1/machine learning/CM1_materials/'\n",
    "\n",
    "# Training data set\n",
    "data_train = pd.read_csv(path + \"mimic_dataset/mimic_train.csv\")         \n",
    "# Testing data set\n",
    "data_test = pd.read_csv(path + \"mimic_dataset/mimic_test_death.csv\")\n",
    "# Diagnoses data\n",
    "data_diagnoses = pd.read_csv(path + \"mimic_dataset/extra_data/MIMIC_diagnoses.csv\")\n",
    "# Ethnicity mapping - map ethnicity to simpler categories (submitted with this notebook)\n",
    "ethnicity_mapping = pd.read_csv(path + 'mimic_dataset/ethnicity.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 - Clean data and generate new features\n",
    "\n",
    "First deal with date columns - `DOB` and `ADMITTIME` - to extract age of patient. This is date of birth of patient and date and time of admission.\n",
    "* Convert to date format using `datetime` package\n",
    "* Add `Diff` column to decode to sensible values\n",
    "* Calculate `AGE` at time of admission as the difference between `ADMITTIME` and `DOB`. Hypothesise that older patients are more likely to die."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [data_train, data_test]:\n",
    "# Convert admittime to date, making adjustment for date scrambling\n",
    "    df['ADMITTIME'] = (df[\"ADMITTIME\"].apply(lambda x: dt.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\"))\n",
    "                      + df[\"Diff\"].apply(lambda x: dt.timedelta(x))).apply(lambda x: x.date())\n",
    "# Convert dob to date, making adjustment for date scrambling\n",
    "    df['DOB'] = (df[\"DOB\"].apply(lambda x: dt.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\"))\n",
    "                + df[\"Diff\"].apply(lambda x: dt.timedelta(x))).apply(lambda x: x.date())\n",
    "# Convert to age in years\n",
    "    df['AGE'] = (df['ADMITTIME'] - df['DOB']).apply(lambda x: x.days//365)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, extract some additional health data for use in the model.\n",
    "\n",
    "**Number of diagnoses per visit (`SEQ_NUM`).** From the (linked) comorbidities dataset, we can extract a count of diagnoses per visit. The idea is that a patient with more diagnoses is more likely to die.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#DIAGNOSES - count diagnoses by patient per hospital visit\n",
    "data_diagnoses.columns = ['subject_id', 'hadm_id', 'SEQ_NUM', 'ICD9_diagnosis']\n",
    "data_diagnoses_per_visit = data_diagnoses[['hadm_id', 'SEQ_NUM']].groupby(['hadm_id'], sort = False).max()\n",
    "\n",
    "# Merge into main data\n",
    "data_train = data_train.merge(data_diagnoses_per_visit, on = ['hadm_id'], how = 'left')\n",
    "data_test = data_test.merge(data_diagnoses_per_visit, on = ['hadm_id'], how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Repeat vists (`repeat_visits`)**. Some patients in the data present to ICU multiple times. Repeat visits may indicate a higher probability of dying. The trick is to calculate cumulative visits *at the time of a particular admission* (i.e. a patient may visit 3 times, but we don't know that on the first or second visit). Calculating this involves ordering the data, grouping by patient and admission identifiers and doing a cumulative count within groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort values by hospital admission and admission date\n",
    "data_train = data_train.sort_values(['subject_id', 'ADMITTIME']).reset_index(drop = True)\n",
    "data_test = data_test.sort_values(['subject_id', 'ADMITTIME']).reset_index(drop = True)\n",
    "\n",
    "#REPEAT VISITS - cumulative total of visits to ICU (known on admission date)\n",
    "data_train['repeat_visits'] = data_train.groupby(['subject_id']).cumcount()+1\n",
    "data_test['repeat_visits'] = data_test.groupby(['subject_id']).cumcount()+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intensive care diagnosis code (`ICD9_diagnosis`)**. There are 1000's of these codes (including one for West Nile Fever with Encephalitis, who knew). For each diagnosis a patient is given, it is mapped to a code. I combine the comorbidities data with information on whether a patient died or not to calculate two kinds of variables. I reasoned that this was allowed because *historical* data on deaths within a diagnosis code are known when a patient enters ICU (i.e. the data is not patient specific, it is diagnosis specific). \n",
    "\n",
    "**Death rates (`max_deathrate`, `mean_deathrate`, `median_deathrate`)**. I calculate death rates for each `ICD9_diagnosis` as the count of deaths in a code divided by the total count of diagnoses in code. Then I assign each patient a maximum, average and median death rate based on all of their comorbidities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BIGGEST KILLERS - looks at the diagnoses with the most deaths (by number and by share)\n",
    "# Join in death indicator\n",
    "data_diagnoses_all = data_diagnoses.merge(data_train[['hadm_id', 'HOSPITAL_EXPIRE_FLAG']], on = ['hadm_id'], how = 'left')\n",
    "# Count deaths and survivals by ICD9_diagnosis\n",
    "biggest_killers = data_diagnoses_all[['HOSPITAL_EXPIRE_FLAG', 'ICD9_diagnosis']].groupby(['HOSPITAL_EXPIRE_FLAG', 'ICD9_diagnosis']).size().reset_index(name='counts').pivot(index = 'ICD9_diagnosis', columns = 'HOSPITAL_EXPIRE_FLAG', values = 'counts')\n",
    "biggest_killers.columns = ['0', '1']\n",
    "# Fill na ICD9 codes with 0's (i.e. no one died from something)\n",
    "biggest_killers = biggest_killers.fillna(0)\n",
    "# Calculate death rates\n",
    "biggest_killers['morbidity_share'] = biggest_killers['1']/(biggest_killers['0'] + biggest_killers['1'])\n",
    "biggest_killers = biggest_killers.reset_index(drop = False)\n",
    "# Merge death rates in with comorbidities data\n",
    "data_diagnoses_all = data_diagnoses_all.merge(biggest_killers[['ICD9_diagnosis', 'morbidity_share']], on = 'ICD9_diagnosis', how = 'left')\n",
    "\n",
    "# Max, mean and median death rates per patient \n",
    "data_death_rates = data_diagnoses_all.groupby(['hadm_id']).agg(max_deathrate = pd.NamedAgg('morbidity_share', 'max'),\n",
    "                                                               mean_deathrate = pd.NamedAgg('morbidity_share', 'mean'),\n",
    "                                                               median_deathrate = pd.NamedAgg('morbidity_share', 'median'))\n",
    "\n",
    "# Merge into main data\n",
    "data_train = data_train.merge(data_death_rates, on = ['hadm_id'], how = 'left')\n",
    "data_test = data_test.merge(data_death_rates, on = ['hadm_id'], how = 'left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most common diagnoses (dummies)**. I also calculated the most common `ICD9_diagnosis` codes among patients that died. I rank each `ICD9_code` based on the frequency in each group of patients (died vs survived). I take 'commonality' as the absolute difference between ranks in each group and scale it by frequency in the (larger) class of patients who survived. The scaling penalises the inclusion of codes where a diagnosis was very common in the survived group, but very rare among patients who died. To further guard against the inclusion of this type of diagnosis, I only keep diagnoses where at least 200 patients in the training data died (this number is arbitrary and can be tuned). \n",
    "\n",
    "The intuition behind this approach (rather than the simpler one of taking the highest frequency codes for patients who died) is that we want to extract codes that most differentiate (maximise the distance) between patients who died and who survived. This process results in 37 codes, which I include as dummies. The benefit of these features above and beyond **death rates** is that they allow the model to account for the *interaction* between comorbidites that are more likely to result in death. This is a feature of the data that scalar death rates cannot capture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_92000/2928800318.py:13: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  most_common_diag = most_common_diag.sort_values(by = 'range', ascending = False)[most_common_diag['counts_x'] > 200]\n",
      "/tmp/ipykernel_92000/2928800318.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  most_common_diag_dummy['flag'] = 1\n"
     ]
    }
   ],
   "source": [
    "# Most common diagnoses for patients that died versus survived\n",
    "most_common_diag_1 = data_diagnoses_all[data_diagnoses_all['HOSPITAL_EXPIRE_FLAG'] == 1].groupby(['ICD9_diagnosis']).size().reset_index(name='counts').sort_values(by = 'counts', ascending = False)                  \n",
    "most_common_diag_0 = data_diagnoses_all[data_diagnoses_all['HOSPITAL_EXPIRE_FLAG'] == 0].groupby(['ICD9_diagnosis']).size().reset_index(name='counts').sort_values(by = 'counts', ascending = False)                  \n",
    "\n",
    "# Add a rank variable \n",
    "for df in [most_common_diag_1, most_common_diag_0]:\n",
    "    df['rank'] = range(1, len(df)+1)\n",
    "\n",
    "# Combine data from each group and calculate commonality indicator abs(rank died - rank survived)/frequency survived \n",
    "most_common_diag = most_common_diag_1.merge(most_common_diag_0, on = 'ICD9_diagnosis', how = 'left')\n",
    "most_common_diag['range'] = np.absolute((most_common_diag['rank_x'] - most_common_diag['rank_y'])/most_common_diag['counts_y'])\n",
    "# Exclude diagnoses where < 200 patients died (can tune)\n",
    "most_common_diag = most_common_diag.sort_values(by = 'range', ascending = False)[most_common_diag['counts_x'] > 200]\n",
    "\n",
    "# Create dummies for most common diagnosis codes in comorbitidites data\n",
    "# Filter for relevant diagnosis codes\n",
    "most_common_diag_dummy = data_diagnoses_all[data_diagnoses_all['ICD9_diagnosis'].isin(list(most_common_diag['ICD9_diagnosis']))]\n",
    "# Add indicator for the diagnosis\n",
    "most_common_diag_dummy['flag'] = 1\n",
    "#There were some duplicate rows in co-morbidities data - drop them\n",
    "dupes = most_common_diag_dummy.drop(['SEQ_NUM', 'HOSPITAL_EXPIRE_FLAG'], axis = 1).duplicated()\n",
    "# Pivot codes to dummies - fill na with zero (didn't have that diagnosis)\n",
    "most_common_diag_dummy = most_common_diag_dummy[~dupes].pivot(index = ['hadm_id'], columns = 'ICD9_diagnosis', values = 'flag').fillna(value = 0)\n",
    "\n",
    "# Merge into main data set\n",
    "data_train = data_train.merge(most_common_diag_dummy, on = ['hadm_id'], how = 'left')\n",
    "data_test = data_test.merge(most_common_diag_dummy, on = ['hadm_id'], how = 'left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, edit the `ETHNICITY` variable. To do this, we map ethnicity from ~40 detailed categories to 5 simpler ones (this follows standard race classification in US): white, asian, black, hispanic, other, not reported. This mapping is saved in an external file, which I have turned in with this notebook. The new variable is called `ETHNICITY_MAP`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.merge(ethnicity_mapping, on = 'ETHNICITY')\n",
    "data_test = data_test.merge(ethnicity_mapping, on = 'ETHNICITY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 - Data exploration\n",
    "Given the features calculated above and the high prevalence multiple categorical variables in the data, I wanted to do some exploratory data analysis. It turns out that this was also useful for thinking about missingness in some of the data. Below I look at the shares of patients who died/survived across many of the features in our the feature space.\n",
    "\n",
    "**Summary**\n",
    "* None of `GENDER`, , `MARITAL_STATUS` and `repeat_visits` showed much variation in the death rate between values\n",
    "* `INSURANCE` showed a higher death rate for patients with Medicare coverage, but this probably reflects that it is a program for the elderly\n",
    "* `FIRST_CAREUNIT` exhibited some variation from the categories: CSRU and MICU\n",
    "* For `ADMISSION_TYPE`, EMERGENCY and URGENT admissions had a much higher rate of death than elective visits (EMERGENCY especially so)\n",
    "* Patients whose `RELIGION`, `ETHNICITY` or `MARITAL_STATUS` was flagged as 'unknown' or 'not recorded' had a much higher incidence of death than other categories. I think this is the missing data telling us something - I hypothesise that a disproportionate share of these patients died before the hospital had an opportunity to record this (periphery) information. To make sure this is captured, I construct an indicator variable based on whether `RELIGION` was recorded or not (I picked this feature because it was a feature I would drop otherwise).\n",
    "\n",
    "You can view any of the diagnostics by changing the last line of the next code block. I left it on `RELIGION` as a default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>share</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RELIGION</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7TH DAY ADVENTIST</th>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BUDDHIST</th>\n",
       "      <td>97.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.110092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CATHOLIC</th>\n",
       "      <td>6826.0</td>\n",
       "      <td>829.0</td>\n",
       "      <td>0.108295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHRISTIAN SCIENTIST</th>\n",
       "      <td>149.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.091463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EPISCOPALIAN</th>\n",
       "      <td>264.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GREEK ORTHODOX</th>\n",
       "      <td>158.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.112360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HEBREW</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HINDU</th>\n",
       "      <td>33.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.131579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JEHOVAH'S WITNESS</th>\n",
       "      <td>40.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JEWISH</th>\n",
       "      <td>1606.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.127174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MUSLIM</th>\n",
       "      <td>66.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.108108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOT SPECIFIED</th>\n",
       "      <td>4862.0</td>\n",
       "      <td>536.0</td>\n",
       "      <td>0.099296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTHER</th>\n",
       "      <td>636.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.094017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROTESTANT QUAKER</th>\n",
       "      <td>2463.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>0.105340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROMANIAN EAST. ORTH</th>\n",
       "      <td>38.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.073171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UNITARIAN-UNIVERSALIST</th>\n",
       "      <td>46.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.148148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UNOBTAINABLE</th>\n",
       "      <td>1227.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>0.190099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0      1     share\n",
       "RELIGION                                       \n",
       "7TH DAY ADVENTIST         29.0    1.0  0.033333\n",
       "BUDDHIST                  97.0   12.0  0.110092\n",
       "CATHOLIC                6826.0  829.0  0.108295\n",
       "CHRISTIAN SCIENTIST      149.0   15.0  0.091463\n",
       "EPISCOPALIAN             264.0   24.0  0.083333\n",
       "GREEK ORTHODOX           158.0   20.0  0.112360\n",
       "HEBREW                     NaN    1.0       NaN\n",
       "HINDU                     33.0    5.0  0.131579\n",
       "JEHOVAH'S WITNESS         40.0    5.0  0.111111\n",
       "JEWISH                  1606.0  234.0  0.127174\n",
       "MUSLIM                    66.0    8.0  0.108108\n",
       "NOT SPECIFIED           4862.0  536.0  0.099296\n",
       "OTHER                    636.0   66.0  0.094017\n",
       "PROTESTANT QUAKER       2463.0  290.0  0.105340\n",
       "ROMANIAN EAST. ORTH       38.0    3.0  0.073171\n",
       "UNITARIAN-UNIVERSALIST    46.0    8.0  0.148148\n",
       "UNOBTAINABLE            1227.0  288.0  0.190099"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EYEBALL DIAGNOSTICS - COLUMNS TO CHECK\n",
    "diagnostics_cols = ['GENDER', 'ETHNICITY_MAP', 'FIRST_CAREUNIT', 'ADMISSION_TYPE', 'INSURANCE',\n",
    "                       'MARITAL_STATUS', 'RELIGION', 'SEQ_NUM', 'repeat_visits']\n",
    "\n",
    "# For loop to calculate share of patients that died in each category\n",
    "diagnostics_eyeball = {}\n",
    "for i in diagnostics_cols:\n",
    "    diagnostic = data_train[['HOSPITAL_EXPIRE_FLAG', i]].groupby(['HOSPITAL_EXPIRE_FLAG', i]).size().reset_index(name='counts').pivot(index = i, columns = 'HOSPITAL_EXPIRE_FLAG', values = 'counts')\n",
    "    diagnostic.columns = ['0', '1']\n",
    "    diagnostic['share'] = diagnostic['1']/(diagnostic['0'] + diagnostic['1'])\n",
    "    diagnostics_eyeball[i] = diagnostic\n",
    "\n",
    "# Add new column for RELIGION\n",
    "for df in [data_train, data_test]:\n",
    "# RELIGION - keep only the UNOBTAINABLE flag\n",
    "    df['RELIGION_ADJ'] = pd.Series([1 if i == 'UNOBTAINABLE' else 0 for i in list(data_train['RELIGION'])])\n",
    "\n",
    "diagnostics_eyeball['RELIGION']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, drop columns that are either not useful or were not available on first day patient entered ICU. Note some columns also not available in test data.\n",
    "* Identifier for test: `icustay_id`\n",
    "* Not useful/too granular: `subject_id`, `hadm_id`, `DIAGNOSIS`\n",
    "* Not available: `DOD`, `DISCHTIME`, `DEATHTIME`, `LOS`\n",
    "* Transformed: `Diff`, `DOB`, `ETHNICITY`, `RELIGION`\n",
    "\n",
    "NOTE THIS IS BASED ON THE ORIGINAL DATA YOU GAVE US - IF YOU HAVE SUBSEQUENTLY REMOVED THESE COLUMNS THE CODE WON'T RUN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID to join with predictions\n",
    "test_kaggle_id = pd.DataFrame(data_test[['icustay_id']])\n",
    "\n",
    "#Drop columns from train and test\n",
    "data_train = data_train.drop(['icustay_id', 'subject_id', 'hadm_id','RELIGION', 'DIAGNOSIS',\n",
    "                              'ICD9_diagnosis', 'DOD', 'DISCHTIME', 'DEATHTIME', 'LOS', 'ADMITTIME',\n",
    "                              'DOB', 'ETHNICITY', 'Diff'], axis=1)\n",
    "\n",
    "#Check number of columns equal in data sets\n",
    "data_test = data_test.drop(['icustay_id', 'subject_id', 'hadm_id', 'RELIGION', 'DIAGNOSIS',\n",
    "                            'ICD9_diagnosis', 'ADMITTIME', 'DOB', 'Diff',\n",
    "                            'ETHNICITY'], axis = 1)\n",
    "\n",
    "len(data_train.columns) - 1 == len(data_test.columns)\n",
    "\n",
    "# Split into training and test data\n",
    "target_col = 'HOSPITAL_EXPIRE_FLAG'\n",
    "\n",
    "# Training data\n",
    "X_train = data_train.drop([target_col], axis=1) \n",
    "y_train = data_train[target_col] \n",
    "\n",
    "# Test data\n",
    "X_test = data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 - diagnostics on missingness and class imbalance\n",
    "\n",
    "It is important to check for nulls in our data. If there are too many nulls for a variable, there is no point imputing because there is not enough variation in the data to give a useful signal. On the other hand, the presence of a null may provide useful information in certain contexts (as seen with unknown observations in `RELIGION`, `ETHNICITY` and `MARITAL_STATUS`). The features with null data (those measuring a patient's vitals) are manageable. I found nothing to suggest they were not 'missing at random', and so will impute their values using a KNN algorithm (to preserve non-linearity of the KNN classifier, but this is also fine for the SVM as well).\n",
    "\n",
    "I also check for class imbalance. It turns out that there could be an issue here with the proportion of deaths in the data set considerably lower than those who survived.\n",
    "\n",
    "For KNN, I correct for this class imbalance in the pipeline using the SMOTE algorithm (I also tried SMOTE-NC, which apparently works better with categorical data, but the difference was negligible and using it was more awkward within the Pipeline). Below I set up the function to reweight the predicted probabilties from the KNN classifier to make them unbiased. For SVC, class imbalance can be handled from within the classifier by assigning class specific weights to the penalty component of the objective function (which will penalise the mis-classification of the minority class more heavily).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count nulls\n",
    "nulls_train = data_train.isnull().sum()\n",
    "nulls_test = data_test.isnull().sum()\n",
    "\n",
    "nulls_train.head()\n",
    "\n",
    "#Check for class imbalance\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "class_counts = dict(zip(unique, counts))\n",
    "\n",
    "# Calculate class weights\n",
    "class0 = class_counts[0]/(class_counts[0] + class_counts[1])\n",
    "class1 = class_counts[1]/(class_counts[0] + class_counts[1])\n",
    "\n",
    "#Function for re-weighting probabilities to correct for class imbalance. \n",
    "def reweight(pi,q1=class1,r1=0.5):\n",
    "    r0 = 1-r1\n",
    "    q0 = 1-q1\n",
    "    tot = pi*(q1/r1)+(1-pi)*(q0/r0)\n",
    "    w = pi*(q1/r1)\n",
    "    w /= tot\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbours\n",
    "\n",
    "Implement the standard sklearn pipeline for the KNN (and later SVC) models. This includes:\n",
    "* Dummifying any categorical variables\n",
    "* Imputing missing data\n",
    "* Scaling data into standardised values\n",
    "* Correcting for an imbalanced sample (KNN only)\n",
    "* Feature selection\n",
    "* Running the classifier model (tuning parameters using `GridSearchCV()`)  \n",
    "\n",
    "Set up the pipeline and specify a standard grid of parameters to search over. An advantage of using the pipeline is that it allows tuning for all of the parameters in the process, not just those specific to the final classifier.\n",
    "\n",
    "The metric to evaluate the model on is set to roc_auc, the same as Kaggle.\n",
    "\n",
    "For **feature selection**, I use recursive feature selection with a Decision Tree Classifier. I chose this approach because the Decision Tree Classifier is a non-linear model that can give some measure of feature importance. I was reticent to fit a linear model to the data for feature selection (for KNN), because this may have excluded features with a meaningful non-linear relationship with the target variable. Recursive Feature Selection with a Decision Tree is a greedy algorithm and takes quite a while to run, but gives good results.\n",
    "\n",
    "**Other things I tried in addition to pipeline below:**\n",
    "* Imputation using `SimpleImputer()` and mean and median strategies (also applies to SVM)\n",
    "* Oversampling using `RandomOverSampler()`\n",
    "* Feature selection using `SelectFromModel()` and `LogisticRegressionCV()`, I ditched this because it was linear.\n",
    "* Different distance weighting approaches in the `KNeighborsClassifier()`: uniform (no distance weighting), Manhattan distance (selected), Euclidean distance and higher order Minkowski distances (p = 3 and p =5). All gave similar results. I also noted the DistanceMetrics page and noted other approaches available for integer- and boolean-valued vector spaces.\n",
    "\n",
    "NOTE: Below I have just included a minimal example of the pipeline (excluding feature selection), so you can see that the code works. As a result, before the pipeline I drop additional variables that feature selection did not include in the model. In all, I end up with about 70 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV 1/5] END ..........imputer__n_neighbors=100;, score=0.871 total time=  50.3s\n",
      "[CV 2/5] END ..........imputer__n_neighbors=100;, score=0.909 total time=  47.0s\n",
      "[CV 3/5] END ..........imputer__n_neighbors=100;, score=0.899 total time=  57.2s\n",
      "[CV 4/5] END ..........imputer__n_neighbors=100;, score=0.899 total time=  52.8s\n",
      "[CV 5/5] END ..........imputer__n_neighbors=100;, score=0.926 total time=  53.5s\n",
      "[CV 1/5] END classifier__n_neighbors=300, classifier__p=1, classifier__weights=distance;, score=0.912 total time=  54.3s\n",
      "[CV 2/5] END classifier__n_neighbors=300, classifier__p=1, classifier__weights=distance;, score=0.928 total time=  55.5s\n",
      "[CV 3/5] END classifier__n_neighbors=300, classifier__p=1, classifier__weights=distance;, score=0.927 total time=  50.7s\n",
      "[CV 4/5] END classifier__n_neighbors=300, classifier__p=1, classifier__weights=distance;, score=0.932 total time=  51.4s\n",
      "[CV 5/5] END classifier__n_neighbors=300, classifier__p=1, classifier__weights=distance;, score=0.948 total time=  46.6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('dummy',\n",
       "                                        ColumnTransformer(remainder='passthrough',\n",
       "                                                          transformers=[('onehotencoder',\n",
       "                                                                         OneHotEncoder(drop='if_binary'),\n",
       "                                                                         ['ADMISSION_TYPE'])])),\n",
       "                                       ('imputer',\n",
       "                                        KNNImputer(n_neighbors=100,\n",
       "                                                   weights='distance')),\n",
       "                                       ('preprocessing', StandardScaler()),\n",
       "                                       ('sampling', SMOTE()),\n",
       "                                       ('classifier',\n",
       "                                        KNeighborsClassifier(n_neighbors=20,\n",
       "                                                             weights='distance'))]),\n",
       "             param_grid=[{'imputer__n_neighbors': [100]},\n",
       "                         {'classifier__n_neighbors': [300],\n",
       "                          'classifier__p': [1],\n",
       "                          'classifier__weights': ['distance']}],\n",
       "             scoring='roc_auc', verbose=3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make column transformer for one hot encoding\n",
    "column_dummy = make_column_transformer((preprocessing.OneHotEncoder(drop = 'if_binary'),\n",
    "                                        ['ADMISSION_TYPE']),\n",
    "                                        #'FIRST_CAREUNIT', 'INSURANCE', 'GENDER',\n",
    "                                        # 'MARITAL_STATUS', 'ETHNICITY_MAP']),\n",
    "                                        remainder = 'passthrough') \n",
    "\n",
    "# Drop variables not included in feature selection:\n",
    "X_train = X_train.drop(['ETHNICITY_MAP', 'INSURANCE', 'MARITAL_STATUS','FIRST_CAREUNIT', 'GENDER'], axis=1)\n",
    "X_test = X_test.drop(['ETHNICITY_MAP', 'INSURANCE', 'MARITAL_STATUS','FIRST_CAREUNIT', 'GENDER'], axis=1)\n",
    "\n",
    "#Check number of columns equal\n",
    "X_train.columns == X_test.columns\n",
    "\n",
    "#Pipeline for KNN\n",
    "pipe_knn = imbPipeline([('dummy', column_dummy), \n",
    "                ('imputer', KNNImputer(missing_values=np.nan, n_neighbors = 100, weights = 'distance', add_indicator = False)),\n",
    "                ('preprocessing', preprocessing.StandardScaler()),\n",
    "                ('sampling', SMOTE()),\n",
    "                #('features', fs.RFECV(estimator = DecisionTreeClassifier(class_weight = 'balanced'),\n",
    "                #                      step = 10, cv = 5, scoring = 'roc_auc', verbose = 0)),\n",
    "                ('classifier', KNeighborsClassifier(n_neighbors = 20,\n",
    "                                                    weights = 'distance',\n",
    "                                                    algorithm = 'auto'))])\n",
    "\n",
    "# Grid\n",
    "grid_values = [{'imputer__n_neighbors':[100]},\n",
    "               {'classifier__n_neighbors':[300],\n",
    "               'classifier__weights':['distance'], 'classifier__p':[1]}]\n",
    "               \n",
    "#Run grid search\n",
    "grid_knn = GridSearchCV(pipe_knn, param_grid = grid_values, scoring = 'roc_auc', cv = 5, verbose = 3)\n",
    "\n",
    "\n",
    "#Fit model\n",
    "grid_knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret Output - KNN\n",
    "\n",
    "Here I assessed the model chosen using grid search. When I was conducting feature selection, I would inspect the number and ranking of the features. Most runs of feature selection selected most of the health features, one indicator of missingness from `MARTIAL_STATUS` or `ETHNICITY` (I ended up using a similar feature with `RELIGION`).\n",
    "\n",
    "Health features includes:\n",
    "* Patient vitals (heart rate, bloos pressure, etc.). I thought about reducing dimensionality here by turning max and min into a range (retaining mean as a levels measure), but you can only do so much.\n",
    "* Number of diagnoses\n",
    "* Cumulative count of visits\n",
    "* Most common diagnosis indicators (sometimes a couple were dropped)\n",
    "* Death rates (most times all three measures - max, mean, median were retained)\n",
    "\n",
    "In all, I include 70 features in the model. The optimal number of neighbours for imputation was 100 and for the classifier it was 300. I used distance weights with the Manhattan distance. The table below shows the mean CV score for this combination, which is quite high.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[70,\n",
       " array(['onehotencoder__ADMISSION_TYPE_ELECTIVE',\n",
       "        'onehotencoder__ADMISSION_TYPE_EMERGENCY',\n",
       "        'onehotencoder__ADMISSION_TYPE_URGENT', 'remainder__HeartRate_Min',\n",
       "        'remainder__HeartRate_Max', 'remainder__HeartRate_Mean',\n",
       "        'remainder__SysBP_Min', 'remainder__SysBP_Max',\n",
       "        'remainder__SysBP_Mean', 'remainder__DiasBP_Min',\n",
       "        'remainder__DiasBP_Max', 'remainder__DiasBP_Mean',\n",
       "        'remainder__MeanBP_Min', 'remainder__MeanBP_Max',\n",
       "        'remainder__MeanBP_Mean', 'remainder__RespRate_Min',\n",
       "        'remainder__RespRate_Max', 'remainder__RespRate_Mean',\n",
       "        'remainder__TempC_Min', 'remainder__TempC_Max',\n",
       "        'remainder__TempC_Mean', 'remainder__SpO2_Min',\n",
       "        'remainder__SpO2_Max', 'remainder__SpO2_Mean',\n",
       "        'remainder__Glucose_Min', 'remainder__Glucose_Max',\n",
       "        'remainder__Glucose_Mean', 'remainder__AGE', 'remainder__SEQ_NUM',\n",
       "        'remainder__repeat_visits', 'remainder__max_deathrate',\n",
       "        'remainder__mean_deathrate', 'remainder__median_deathrate',\n",
       "        'remainder__0389', 'remainder__2449', 'remainder__25000',\n",
       "        'remainder__2724', 'remainder__2760', 'remainder__2761',\n",
       "        'remainder__2762', 'remainder__2767', 'remainder__2851',\n",
       "        'remainder__2859', 'remainder__2875', 'remainder__4019',\n",
       "        'remainder__40390', 'remainder__41401', 'remainder__42731',\n",
       "        'remainder__4275', 'remainder__4280', 'remainder__486',\n",
       "        'remainder__496', 'remainder__5070', 'remainder__5119',\n",
       "        'remainder__51881', 'remainder__53081', 'remainder__570',\n",
       "        'remainder__5845', 'remainder__5849', 'remainder__5859',\n",
       "        'remainder__5990', 'remainder__78552', 'remainder__78959',\n",
       "        'remainder__99592', 'remainder__V1582', 'remainder__V4581',\n",
       "        'remainder__V4986', 'remainder__V5861', 'remainder__V667',\n",
       "        'remainder__RELIGION_ADJ'], dtype=object)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DIAGNOSTICS\n",
    "\n",
    "#Features\n",
    "# Feature ranking\n",
    "#feature_ranking = pd.DataFrame(list(grid_knn.best_estimator_.named_steps['dummy'].get_feature_names_out()),\n",
    "#                               list(grid_knn.best_estimator_.named_steps['features'].ranking_)).reset_index(drop = False).sort_values(by = 'index')\n",
    "# Features chosen\n",
    "#feature_support = list(grid_knn.best_estimator_.named_steps['features'].support_)\n",
    "#pd.DataFrame(grid_knn.best_estimator_.named_steps['dummy'].get_feature_names_out())[feature_support]\n",
    "\n",
    "[grid_knn.best_estimator_.named_steps['classifier'].n_features_in_, grid_knn.best_estimator_.named_steps['dummy'].get_feature_names_out()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_name</th>\n",
       "      <th>param_values</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[classifier__n_neighbors, classifier__p, class...</td>\n",
       "      <td>300_1_distance</td>\n",
       "      <td>1</td>\n",
       "      <td>0.929217</td>\n",
       "      <td>0.011455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[imputer__n_neighbors]</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>0.900637</td>\n",
       "      <td>0.017689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          param_name    param_values  \\\n",
       "1  [classifier__n_neighbors, classifier__p, class...  300_1_distance   \n",
       "0                             [imputer__n_neighbors]             100   \n",
       "\n",
       "   rank_test_score  mean_test_score  std_test_score  \n",
       "1                1         0.929217        0.011455  \n",
       "0                2         0.900637        0.017689  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model parameters\n",
    "# KNN imputer\n",
    "grid_knn.best_estimator_.named_steps['imputer'].get_params()\n",
    "# KNN classifier\n",
    "results_knn = pd.DataFrame(grid_knn.cv_results_)\n",
    "results_knn = results_knn.sort_values(by=[\"rank_test_score\"])\n",
    "results_knn[\"param_values\"] = results_knn[\"params\"].apply(lambda x: \"_\".join(str(val) for val in x.values()))\n",
    "results_knn['param_name'] = [list(i.keys()) for i in results_knn[\"params\"]]\n",
    "results_knn[[\"param_name\", \"param_values\", \"rank_test_score\", \"mean_test_score\", \"std_test_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate predictions and write to a CSV for upload to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. PREDICT USING TEST DATA\n",
    "y_pred_prob = grid_knn.predict_proba(X_test)\n",
    "\n",
    "#If reweighted for class imbalance\n",
    "y_pred_prob = reweight(y_pred_prob)\n",
    "\n",
    "#Write TO CSV FOR KAGGLE\n",
    "test_kaggle = pd.concat([test_kaggle_id, pd.Series(y_pred_prob[:,1]).rename('HOSPITAL_EXPIRE_FLAG')], axis = 1) ## The unique ID\n",
    "test_kaggle.head()\n",
    "test_kaggle.to_csv(path + \"gigi_kaggle_knn.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "Set up the pipeline - which is largely similar to the KNN pipeline. I have left a more comprehensive parameter search below, I searched across different penalisation parameters for misclassified data (C), kernels (linear and radial basis function), class weights (balanced and None). Gamma is the tuning parameter for the radial basis function kernel. I did not specify more parameters because SVM took a long time to run on my computer.\n",
    "\n",
    "I set probability = True in the classifier to allow for the retrieval of predicted probabilities. This fits a logistic regression to the model output to calculate probability estimates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV 1/5] END ..........imputer__n_neighbors=100;, score=0.920 total time= 1.4min\n",
      "[CV 2/5] END ..........imputer__n_neighbors=100;, score=0.948 total time= 1.4min\n",
      "[CV 3/5] END ..........imputer__n_neighbors=100;, score=0.940 total time= 1.4min\n",
      "[CV 4/5] END ..........imputer__n_neighbors=100;, score=0.947 total time= 1.5min\n",
      "[CV 5/5] END ..........imputer__n_neighbors=100;, score=0.966 total time= 1.6min\n",
      "[CV 1/5] END classifier__C=1, classifier__class_weight=balanced, classifier__kernel=linear;, score=0.934 total time= 3.4min\n",
      "[CV 2/5] END classifier__C=1, classifier__class_weight=balanced, classifier__kernel=linear;, score=0.952 total time= 3.6min\n",
      "[CV 3/5] END classifier__C=1, classifier__class_weight=balanced, classifier__kernel=linear;, score=0.946 total time= 3.5min\n",
      "[CV 4/5] END classifier__C=1, classifier__class_weight=balanced, classifier__kernel=linear;, score=0.953 total time= 3.6min\n",
      "[CV 5/5] END classifier__C=1, classifier__class_weight=balanced, classifier__kernel=linear;, score=0.969 total time= 3.7min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('dummy',\n",
       "                                        ColumnTransformer(remainder='passthrough',\n",
       "                                                          transformers=[('onehotencoder',\n",
       "                                                                         OneHotEncoder(drop='if_binary'),\n",
       "                                                                         ['ADMISSION_TYPE'])])),\n",
       "                                       ('imputer',\n",
       "                                        KNNImputer(n_neighbors=100,\n",
       "                                                   weights='distance')),\n",
       "                                       ('preprocessing', StandardScaler()),\n",
       "                                       ('classifier', SVC(probability=True))]),\n",
       "             param_grid=[{'imputer__n_neighbors': [100]},\n",
       "                         {'classifier__C': [1],\n",
       "                          'classifier__class_weight': ['balanced'],\n",
       "                          'classifier__kernel': ['linear']}],\n",
       "             scoring='roc_auc', verbose=3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pipeline for SVC model\n",
    "pipe = Pipeline([('dummy', column_dummy), \n",
    "                ('imputer', KNNImputer(missing_values=np.nan, n_neighbors = 100, weights = 'distance', add_indicator = False)),\n",
    "                ('preprocessing', preprocessing.StandardScaler()),             \n",
    "                ('classifier', SVC(probability=True))])\n",
    "\n",
    "#grid_values = [{'imputer__n_neighbors':[100]},\n",
    "#               {'classifier__C':[0.01, 1, 10],\n",
    "#               'classifier__class_weight':[\"balanced\", None],\n",
    "#               'classifier__kernel':['linear', 'rbf'],\n",
    "#              'classifier__gamma':[0.25, 0.5, 0.75]}]\n",
    "\n",
    "grid_values = [{'imputer__n_neighbors':[100]},\n",
    "               {'classifier__C':[1],\n",
    "               'classifier__class_weight':[\"balanced\"],\n",
    "               'classifier__kernel':['linear']}]\n",
    "\n",
    "#Run grid search\n",
    "grid_svm = GridSearchCV(pipe, param_grid = grid_values, scoring = 'roc_auc', cv=5, verbose = 3)\n",
    "\n",
    "#4. FIT MODEL TO DATA\n",
    "grid_svm.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret output - SVM\n",
    "\n",
    "Similar to the KNN model, I included around 70 features. GridSearch preferred a linear kernel and balanced class weights. The optimal value for C over the values I tried was 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[70,\n",
       " array(['onehotencoder__ADMISSION_TYPE_ELECTIVE',\n",
       "        'onehotencoder__ADMISSION_TYPE_EMERGENCY',\n",
       "        'onehotencoder__ADMISSION_TYPE_URGENT', 'remainder__HeartRate_Min',\n",
       "        'remainder__HeartRate_Max', 'remainder__HeartRate_Mean',\n",
       "        'remainder__SysBP_Min', 'remainder__SysBP_Max',\n",
       "        'remainder__SysBP_Mean', 'remainder__DiasBP_Min',\n",
       "        'remainder__DiasBP_Max', 'remainder__DiasBP_Mean',\n",
       "        'remainder__MeanBP_Min', 'remainder__MeanBP_Max',\n",
       "        'remainder__MeanBP_Mean', 'remainder__RespRate_Min',\n",
       "        'remainder__RespRate_Max', 'remainder__RespRate_Mean',\n",
       "        'remainder__TempC_Min', 'remainder__TempC_Max',\n",
       "        'remainder__TempC_Mean', 'remainder__SpO2_Min',\n",
       "        'remainder__SpO2_Max', 'remainder__SpO2_Mean',\n",
       "        'remainder__Glucose_Min', 'remainder__Glucose_Max',\n",
       "        'remainder__Glucose_Mean', 'remainder__AGE', 'remainder__SEQ_NUM',\n",
       "        'remainder__repeat_visits', 'remainder__max_deathrate',\n",
       "        'remainder__mean_deathrate', 'remainder__median_deathrate',\n",
       "        'remainder__0389', 'remainder__2449', 'remainder__25000',\n",
       "        'remainder__2724', 'remainder__2760', 'remainder__2761',\n",
       "        'remainder__2762', 'remainder__2767', 'remainder__2851',\n",
       "        'remainder__2859', 'remainder__2875', 'remainder__4019',\n",
       "        'remainder__40390', 'remainder__41401', 'remainder__42731',\n",
       "        'remainder__4275', 'remainder__4280', 'remainder__486',\n",
       "        'remainder__496', 'remainder__5070', 'remainder__5119',\n",
       "        'remainder__51881', 'remainder__53081', 'remainder__570',\n",
       "        'remainder__5845', 'remainder__5849', 'remainder__5859',\n",
       "        'remainder__5990', 'remainder__78552', 'remainder__78959',\n",
       "        'remainder__99592', 'remainder__V1582', 'remainder__V4581',\n",
       "        'remainder__V4986', 'remainder__V5861', 'remainder__V667',\n",
       "        'remainder__RELIGION_ADJ'], dtype=object)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model features\n",
    "\n",
    "[grid_svm.best_estimator_.named_steps['classifier'].n_features_in_, grid_svm.best_estimator_.named_steps['dummy'].get_feature_names_out()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_name</th>\n",
       "      <th>param_values</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[classifier__C, classifier__class_weight, clas...</td>\n",
       "      <td>1_balanced_linear</td>\n",
       "      <td>1</td>\n",
       "      <td>0.950930</td>\n",
       "      <td>0.011333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[imputer__n_neighbors]</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>0.944266</td>\n",
       "      <td>0.014680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          param_name       param_values  \\\n",
       "1  [classifier__C, classifier__class_weight, clas...  1_balanced_linear   \n",
       "0                             [imputer__n_neighbors]                100   \n",
       "\n",
       "   rank_test_score  mean_test_score  std_test_score  \n",
       "1                1         0.950930        0.011333  \n",
       "0                2         0.944266        0.014680  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model parameters\n",
    "# KNN imputer\n",
    "grid_svm.best_estimator_.named_steps['imputer'].get_params()\n",
    "# SVM classifier\n",
    "results_svm = pd.DataFrame(grid_svm.cv_results_)\n",
    "results_svm = results_svm.sort_values(by=[\"rank_test_score\"])\n",
    "results_svm[\"param_values\"] = results_svm[\"params\"].apply(lambda x: \"_\".join(str(val) for val in x.values()))\n",
    "results_svm['param_name'] = [list(i.keys()) for i in results_svm[\"params\"]]\n",
    "results_svm[[\"param_name\", \"param_values\", \"rank_test_score\", \"mean_test_score\", \"std_test_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. PREDICT USING TEST/TRAIN DATA\n",
    "y_pred_prob_svm = grid_svm.predict_proba(X_test)\n",
    "\n",
    "#Write TO CSV FOR KAGGLE\n",
    "test_kaggle_svm = pd.concat([test_kaggle_id, pd.Series(y_pred_prob_svm[:,1]).rename('HOSPITAL_EXPIRE_FLAG')], axis = 1) ## The unique ID\n",
    "test_kaggle.head()\n",
    "test_kaggle.to_csv(path +\"gigi_kaggle_svm.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
